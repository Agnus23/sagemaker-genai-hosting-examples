{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Quantized FM Models Utilizing SageMaker Clarify and Inference\n",
    "\n",
    "In this sample notebook we use the [SageMaker Inference Optimization Toolkit](https://aws.amazon.com/blogs/machine-learning/achieve-up-to-2x-higher-throughput-while-reducing-costs-by-50-for-generative-ai-inference-on-amazon-sagemaker-with-the-new-inference-optimization-toolkit-part-1/) to showcase how you can quantize a Llama3-70B model and deploy on a smaller GPU instance type. Quantization is a popular model compression technique that uses lower precision data types to reduce the memory footprint and accelerate inference. Via the Inference Optimization Toolkit, [AWQ](https://arxiv.org/abs/2306.00978) is currently supported as a Quantization method. \n",
    "\n",
    "Quantization does have a trade off in the fact that by reducing the precision of the model's parameters this can lead to a decrease in the actual accuracy of the model itself. In this notebook we explore how you can use SageMaker Clarify's Foundation Model Evaluation Tool [(FMEval)](https://github.com/aws/fmeval/tree/main) to evaluate a base and quantized Llama3-70B model to benchmark for accuracy and get a holistic understanding of the accuracy difference of the two models.\n",
    "\n",
    "Note that with the FMEval package, you must pick the NLP task and algorithm of your choice. For the purpose of this notebook we will utilize the Factual Knowledge algorithm, but if you are dealing with a Summarization use-case for instance, ensure that you instantiate that algorithm to reflect your use-case. For more FMEval samples, please refer to this [link](https://github.com/aws/fmeval/tree/main/examples).\n",
    "\n",
    "## Additional Resources\n",
    "- [Inference Optimization Toolkit Blog](https://aws.amazon.com/blogs/machine-learning/achieve-up-to-2x-higher-throughput-while-reducing-costs-by-50-for-generative-ai-inference-on-amazon-sagemaker-with-the-new-inference-optimization-toolkit-part-1/)\n",
    "- [FMEval Intro Blog](https://aws.amazon.com/blogs/machine-learning/evaluate-large-language-models-for-quality-and-responsibility/)\n",
    "- [Model Builder SageMaker Python SDK Class](https://aws.amazon.com/blogs/machine-learning/package-and-deploy-classical-ml-and-llms-easily-with-amazon-sagemaker-part-1-pysdk-improvements/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup & Base Model Deployment\n",
    "\n",
    "For this example we use the Model Builder class via the SageMaker Python SDK to deploy a Llama3-70B model. We will deploy the base Llama model with the default hardware configuration and then quantize the model to deploy on a smaller instance via the Inference Optimization toolkit. All of this will be easily accessible to us via the Model Builder Class which abstracts out container and parameter selection for end users and provides default optimized configurations for popular LLMs such as Llama3-70B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sagemaker>=2.225.0 accelerate boto3 jsonlines fmeval --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from sagemaker.serve.builder.model_builder import ModelBuilder\n",
    "from sagemaker.serve.builder.schema_builder import SchemaBuilder\n",
    "from sagemaker.session import Session\n",
    "import logging\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "\n",
    "artifacts_bucket_name = sagemaker_session.default_bucket()\n",
    "execution_role_arn = sagemaker_session.get_caller_identity_arn()\n",
    "\n",
    "# can specify a JumpStart Model ID for deployment via Model Builder\n",
    "js_model_id = \"meta-textgeneration-llama-3-70b\"\n",
    "gpu_instance_type = \"ml.p4d.24xlarge\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define input/output payload shapes for the Model Builder class\n",
    "response = \"Hello, I'm a language model, and I'm here to help you with your English.\"\n",
    "\n",
    "sample_input = {\n",
    "    \"inputs\": \"Hello, I'm a language model,\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "\n",
    "sample_output = [{\"generated_text\": response}]\n",
    "\n",
    "schema_builder = SchemaBuilder(sample_input, sample_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = model_builder.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictor = base_model.deploy(instance_type = gpu_instance_type, accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_predictor.predict(sample_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "Via the SageMaker Inference Toolkit we can specify AWQ quantization, this will reduce the memory footprint of the model and allow for us to deploy on a smaller GPU instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define quantized model builder class\n",
    "quantized_model_builder = ModelBuilder(\n",
    "    model=js_model_id,\n",
    "    schema_builder=schema_builder,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role_arn=execution_role_arn,\n",
    "    log_level=logging.ERROR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an optimization job can take up to 2 hours you can also view this in the Studio UI\n",
    "optimized_model = quantized_model_builder.optimize(\n",
    "    instance_type=gpu_instance_type,\n",
    "    accept_eula=True,\n",
    "    quantization_config={\n",
    "        \"OverrideEnvironment\": {\n",
    "            \"OPTION_QUANTIZE\": \"awq\",\n",
    "        },\n",
    "    },\n",
    "    output_path=f\"s3://{artifacts_bucket_name}/awq-quantization/\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_instance_type = \"ml.g5.12xlarge\"\n",
    "quantized_predictor = optimized_model.deploy(instance_type=quantized_instance_type, accept_eula=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantized_predictor.predict(sample_input)['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation via SageMaker Foundation Model Evaluations\n",
    "Now that we have both the base model deployed and the quantized model, we can take a deeper look into evaluating both models using the open-source FMEval package. Ensure that you have installed the library and have the requisite built-in datasets locally (attached to this repo as well). In this case we use the Factual Knowledge algorithm to test both models abilities against fact based questions, for this we will use the built-in trex-sample.jsonl dataset.\n",
    "\n",
    "For using FMEval there are two methods:\n",
    "- <b>evaluate_sample</b>: Here you can evaluate a single record by providing the model output against the target (ground truth) output.\n",
    "- <b>evaluate</b>: Pass an entire dataset to run an evaluation across."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmeval.eval_algorithms.factual_knowledge import FactualKnowledge, FactualKnowledgeConfig\n",
    "eval_algo = FactualKnowledge(FactualKnowledgeConfig(\"<OR>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_fact_input = {\n",
    "    \"inputs\": \"Aurillac is the capital of\",\n",
    "    \"parameters\": {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6},\n",
    "}\n",
    "model_output = quantized_predictor.predict(sample_fact_input)['generated_text']\n",
    "model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate a single sample\n",
    "eval_algo.evaluate_sample(target_output=\"Cantal\", model_output=model_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Quantized and Base Model\n",
    "We can take the subset dataset and run inference with both the quantized model and base model to create two datasets for evaluation. We define two helper methods below to create the datasets that we can then run evaluation on with the FMEval package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# Check that the dataset file to be used by the evaluation is present\n",
    "if not glob.glob(\"trex_sample_subset.jsonl\"):\n",
    "    print(\"ERROR - please make sure the file, trex_sample.jsonl, exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import jsonlines\n",
    "runtime_client = boto3.client('sagemaker-runtime')\n",
    "content_type = \"application/json\"\n",
    "\n",
    "def create_payload(prompt: str, parameters: dict = {\"max_new_tokens\": 128, \"top_p\": 0.9, \"temperature\": 0.6}) -> dict:\n",
    "    \"\"\"\n",
    "    Creates a model invocation payload.\n",
    "    \n",
    "    Args:\n",
    "        prompt (str): Prompt for the LLM\n",
    "        parameters (dict): Customizable model invocation parameters\n",
    "    \n",
    "    Returns:\n",
    "        Payload to be used when invoking the model.\n",
    "    \"\"\"\n",
    "    \n",
    "    if len(prompt) == 0:\n",
    "        raise ValueError(\"Please provide a non-empty prompt.\")\n",
    "    \n",
    "    return {\n",
    "        \"inputs\": prompt,\n",
    "        \"parameters\": parameters\n",
    "    }\n",
    "\n",
    "\n",
    "def create_eval_files(endpoint_name: str, model_outputs_file: str, input_file: str = \"trex_sample_subset.jsonl\") -> str:\n",
    "    try:\n",
    "        with jsonlines.open(input_file) as input_fh, jsonlines.open(model_outputs_file, \"w\") as output_fh:\n",
    "            for line in input_fh:\n",
    "                if \"question\" in line:\n",
    "                    question = line[\"question\"]\n",
    "                    payload = create_payload(question)\n",
    "                    #print(payload)\n",
    "                    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(payload), ContentType=content_type)\n",
    "                    model_output = json.loads(response['Body'].read().decode())['generated_text']\n",
    "                    #print(f\"Model output: {model_output}\")\n",
    "                    #print(\"==============================\")\n",
    "                    line[\"model_output\"] = model_output\n",
    "                    output_fh.write(line)\n",
    "    except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "    print(f\"Created Model Outputs File: {model_outputs_file}\")\n",
    "\n",
    "quantized_model_output_file = create_eval_files(endpoint_name = quantized_predictor.endpoint_name, model_outputs_file = \"quantized_model_outputs.jsonl\")\n",
    "base_model_output_file = create_eval_files(endpoint_name = base_predictor.endpoint_name, model_outputs_file = \"base_model_outputs.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data Config\n",
    "For FMEval we need to define Data Config objects that point towards our datasets we just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fmeval.data_loaders.data_config import DataConfig\n",
    "from fmeval.constants import MIME_TYPE_JSONLINES\n",
    "\n",
    "\n",
    "# prepare data config for quantized model\n",
    "quantized_data_config = DataConfig(\n",
    "    dataset_name=\"trex_sample_with_quantized_model_outputs\",\n",
    "    dataset_uri=\"quantized_model_outputs.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answers\",\n",
    "    model_output_location=\"model_output\"\n",
    ")\n",
    "\n",
    "# data config for base model\n",
    "base_data_config = DataConfig(\n",
    "    dataset_name=\"trex_sample_with_base_model_outputs\",\n",
    "    dataset_uri=\"base_model_outputs.jsonl\",\n",
    "    dataset_mime_type=MIME_TYPE_JSONLINES,\n",
    "    model_input_location=\"question\",\n",
    "    target_output_location=\"answers\",\n",
    "    model_output_location=\"model_output\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Methods\n",
    "Once defining our Data Config objects and instantiating our Eval Algorithms, we can run the evaluate method on both the quantized and base model outputs to see the accuracy difference for the Factual Knowledge algorithm. For the results across each datapoint you can check the following directory post evaluation: ```/tmp/eval_results/```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output_quantized = eval_algo.evaluate(dataset_config=quantized_data_config, save=True)\n",
    "print(eval_output_quantized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_output_base = eval_algo.evaluate(dataset_config=base_data_config, save=True)\n",
    "print(eval_output_base)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
