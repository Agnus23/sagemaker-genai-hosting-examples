{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling on LLM hosted on Amazon SageMaker\n",
    "\n",
    "Function calling is the ability to reliably connect LLMs to external tools to enable effective tool usage and interaction with external APIs.\n",
    "\n",
    "Function calling is an important ability for building LLM-powered chatbots or agents that need to retrieve context for an LLM or interact with external tools by converting natural language into API calls.\n",
    "\n",
    "Functional calling enables developers to create:\n",
    "\n",
    "* Conversational agents that can efficiently use external tools to answer questions. For example, the query \"What is the weather like in Belize?\" will be converted to a function call such as get_current_weather(location: string, unit: 'celsius' | 'fahrenheit').\n",
    "* LLM-powered solutions for extracting and tagging data (e.g., extracting people names from a Wikipedia article).\n",
    "* Applications that can help convert natural language to API calls or valid database queries.\n",
    "* Conversational knowledge retrieval engines that interact with a knowledge base.\n",
    "\n",
    "In this example, we will be hosting [Meta-Llama-3-8B](https://huggingface.co/Trelis/Meta-Llama-3-8B-Instruct-function-calling) on sagemaker and invoke it using function calling. \n",
    "\n",
    "Meta released [Llama 3](https://huggingface.co/blog/llama3), the next iteration of the open-access Llama family. Llama 3 comes in two sizes: [8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct) for efficient deployment and development on consumer-size GPU, and [70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-instruct) for large-scale AI native applications. Both come in base and instruction-tuned variants. The vanilla llama3 models doesnot support function calling hence we will be using finetuned model which has support for function calling\n",
    "\n",
    "\n",
    "Lets get started!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup development environment\n",
    "\n",
    "We are going to use the `sagemaker` python SDK to deploy Llama3 to Amazon SageMaker. We need to make sure to have an AWS account configured and the `sagemaker` python SDK installed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "aiobotocore 2.12.2 requires botocore<1.34.52,>=1.34.41, but you have botocore 1.34.135 which is incompatible.\n",
      "awscli 1.33.1 requires botocore==1.34.119, but you have botocore 1.34.135 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::716256856266:role/service-role/AmazonSageMaker-ExecutionRole-20240603T162068\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']\n",
    "\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to deploying regular Hugging Face models we first need to retrieve the container uri and provide it to our `HuggingFaceModel` model class with a `image_uri` pointing to the image. To retrieve the new Hugging Face LLM DLC in Amazon SageMaker, we can use the `get_huggingface_llm_image_uri` method provided by the `sagemaker` SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified `backend`, `session`, `region`, and `version`. You can find the available versions [here](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#huggingface-text-generation-inference-containers)\n",
    "\n",
    "_Note: At the time of writing this blog post the latest version of the Hugging Face LLM DLC is not yet available via the `get_huggingface_llm_image_uri` method. We are going to use the raw container uri instead._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llm image uri: 763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\n"
     ]
    }
   ],
   "source": [
    "# COMMENT IN WHEN PR (https://github.com/aws/sagemaker-python-sdk/pull/4314) IS MERGED\n",
    "# from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# # retrieve the llm image uri\n",
    "# llm_image = get_huggingface_llm_image_uri(\n",
    "#   \"huggingface\",\n",
    "#   version=\"2.0.0\"\n",
    "# )\n",
    "llm_image = f\"763104351884.dkr.ecr.{sess.boto_region_name}.amazonaws.com/huggingface-pytorch-tgi-inference:2.1-tgi2.0-gpu-py310-cu121-ubuntu22.04\"\n",
    "\n",
    "# print ecr image uri\n",
    "print(f\"llm image uri: {llm_image}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hardware requirements\n",
    "\n",
    "Llama 3 comes in 2 different sizes - 8B & 70B parameters. The hardware requirements will vary based on the model size deployed to SageMaker. Below is a set up minimum requirements for each model size we tested.\n",
    "\n",
    "| Model                                                              | Instance Type       | Quantization   | # of GPUs per replica |\n",
    "| ------------------------------------------------------------------ | ------------------- | -------------- | --------------------- |\n",
    "| [Llama 8B](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct)   | `(ml.)g5.2xlarge`   | `-`            | 1                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)g5.12xlarge`  | `gptq` |Â `awq` | 8                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)g5.48xlarge`  | `-` | 8                     |\n",
    "| [Llama 70B](https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct) | `(ml.)p4d.24xlarge` | `-`            | 8                     |\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deploy Llama 3 to Amazon SageMaker\n",
    "\n",
    "To deploy [Llama 3 8B](https://huggingface.co/Trelis/Meta-Llama-3-8B-Instruct-function-calling) to Amazon SageMaker we create a `HuggingFaceModel` model class and define our endpoint configuration including the `hf_model_id`, `instance_type` etc.  Llama 3 8B instruct function calling is  fine-tuned for function calling. We will interact with llama using the common OpenAI format of `messages`. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# sagemaker config\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "health_check_timeout = 900\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define Model and Endpoint configuration parameter\n",
    "config = {\n",
    "  'HF_MODEL_ID': \"Trelis/Meta-Llama-3-8B-Instruct-function-calling\", # model_id from hf.co/models\n",
    "  'SM_NUM_GPUS': \"1\", # Number of GPU used per replica\n",
    "  'MAX_INPUT_LENGTH': \"2048\",  # Max length of input text\n",
    "  'MAX_TOTAL_TOKENS': \"4096\",  # Max length of the generation (including input text)\n",
    "  'MAX_BATCH_TOTAL_TOKENS': \"8192\",  # Limits the number of tokens that can be processed in parallel during the generation\n",
    "  'MESSAGES_API_ENABLED': \"true\", # Enable the messages API\n",
    "  'HUGGING_FACE_HUB_TOKEN': \"\" #Update this\n",
    "}\n",
    "\n",
    "# check if token is set\n",
    "\n",
    "\n",
    "# create HuggingFaceModel with the image uri\n",
    "llm_model = HuggingFaceModel(\n",
    "  role=role,\n",
    "  image_uri=llm_image,\n",
    "  env=config\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have created the `HuggingFaceModel` we can deploy it to Amazon SageMaker using the `deploy` method. We will deploy the model with the `ml.g5.12xlarge` instance type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------!"
     ]
    }
   ],
   "source": [
    "# Deploy model to an endpoint\n",
    "# https://sagemaker.readthedocs.io/en/stable/api/inference/model.html#sagemaker.model.Model.deploy\n",
    "llm = llm_model.deploy(\n",
    "  initial_instance_count=1,\n",
    "  instance_type=instance_type,\n",
    "  container_startup_health_check_timeout=health_check_timeout, # 10 minutes to be able to load the model\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker will now create our endpoint and deploy the model to it. This can takes a 10-15 minutes. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run inference and chat with the model\n",
    "\n",
    "After our endpoint is deployed we can run inference on it. We will use the `predict` method from the `predictor` to run inference on our endpoint. We can inference with different parameters to impact the generation. Parameters can be defined as in the `parameters` attribute of the payload. You can find supported parameters in the [here](https://huggingface.co/docs/text-generation-inference/messages_api). \n",
    "\n",
    "The Messages API allows us to interact with the model in a conversational way. We can define the role of the message and the content. The role can be either `system`,`assistant` or `user`. The `system` role is used to provide context to the model and the `user` role is used to ask questions or provide input to the model. \n",
    "\n",
    "```json\n",
    "{\n",
    "  \"messages\": [\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ],\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt to generate\n",
    "messages=[\n",
    "    { \"role\": \"system\", \"content\": \"You are a helpful assistant.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is deep learning?\" }\n",
    "  ]\n",
    "\n",
    "# Generation arguments\n",
    "parameters = {\n",
    "    \"model\": \"Trelis/Meta-Llama-3-8B-Instruct-function-calling\", # placholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    #\"stop\": [\"<|eot_id|>\"],\n",
    "    #\"tools\"=tools\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay lets test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep learning is a subset of machine learning that involves the use of artificial neural networks to model and analyze data. It is called \"deep\" because it involves multiple layers of interconnected nodes or \"neurons\" that process and transform the data as it flows through the network.\n",
      "\n",
      "In traditional machine learning, algorithms are designed to learn from data by identifying patterns and making predictions based on those patterns. In contrast, deep learning algorithms are designed to learn from data by identifying complex patterns and relationships that are not easily discernible by humans.\n",
      "\n",
      "Deep learning is particularly well-suited for tasks that involve:\n",
      "\n",
      "1. Image recognition: Deep learning algorithms can be trained to recognize objects, scenes, and activities in images and videos.\n",
      "2. Natural language processing: Deep learning algorithms can be trained to understand and generate human language, including text and speech.\n",
      "3. Speech recognition: Deep learning algorithms can be trained to recognize and transcribe spoken language.\n",
      "4. Time series forecasting: Deep learning algorithms can be trained to predict future values in a time series based on past values.\n",
      "\n",
      "The key characteristics of deep learning are:\n",
      "\n",
      "1. Multiple layers: Deep learning algorithms typically involve multiple layers of neurons, each of which processes and transforms the data in a different way.\n",
      "2. Non-linearity: Deep learning algorithms can learn non-linear relationships between inputs and outputs, which allows them to model complex patterns and relationships.\n",
      "3. Auto-encoding: Deep learning algorithms can learn to compress and reconstruct data, which allows them to learn efficient representations of the data.\n",
      "4. Backpropagation: Deep learning algorithms use backpropagation to adjust the weights and biases of the neurons based on the error between the predicted output and the actual output.\n",
      "\n",
      "Some of the benefits of deep learning include:\n",
      "\n",
      "1. Improved accuracy: Deep learning algorithms can achieve high levels of accuracy on complex tasks.\n",
      "2. Flexibility: Deep learning algorithms can be trained on large datasets and can learn to generalize to new data.\n",
      "3. Scalability: Deep learning algorithms can be parallelized and distributed across multiple machines, which allows them to handle large datasets and complex tasks.\n",
      "\n",
      "However, deep learning also has some challenges and limitations, including:\n",
      "\n",
      "1. Data requirements: Deep learning algorithms require large amounts of high-quality data to train and test.\n",
      "2. Computational requirements: Deep learning algorithms can be computationally intensive and require significant resources to train and test.\n",
      "3. Interpretability: Deep learning algorithms can be difficult to interpret and understand, which can make it challenging to identify biases and errors.\n",
      "4. Overfitting: Deep learning algorithms can overfit the training data\n"
     ]
    }
   ],
   "source": [
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling\n",
    "\n",
    "As a basic example, let's say we asked the model to check the weather in a given location \n",
    "\n",
    "The LLM alone would not be able to respond to this request because it has been trained on a dataset with a cutoff point. The way to solve this is to combine the LLM with an external tool. You can leverage the function calling capabilities of the model to determine an external function to call along with its arguments and then have it return a final response. Below is a simple example of how you can achieve this using the Sagemaker's invoke model API.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "FUNCTION_METADATA=[\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"This function gets the current weather in a given city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city, e.g., San Francisco\"\n",
    "                    },\n",
    "                    \"format\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"celsius\", \"fahrenheit\"],\n",
    "                        \"description\": \"The temperature unit to use.\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_clothes\",\n",
    "            \"description\": \"This function provides a suggestion of clothes to wear based on the current weather\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"temperature\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The temperature, e.g., 15 C or 59 F\"\n",
    "                    },\n",
    "                    \"condition\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The weather condition, e.g., 'Cloudy', 'Sunny', 'Rainy'\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"temperature\", \"condition\"]\n",
    "            }\n",
    "        }\n",
    "    }    \n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "    {\n",
    "        \"role\": \"function_metadata\",\n",
    "        \"content\": \"FUNCTION_METADATA\"\n",
    "    },\n",
    "    \n",
    "    {\n",
    "        \"role\": \"function_call\",\n",
    "        \"content\": \"{\\n    \\\"name\\\": \\\"get_current_weather\\\",\\n    \\\"arguments\\\": {\\n        \\\"city\\\": \\\"London\\\"\\n    }\\n}\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What is the current weather in London?\"\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"name\": \"get_current_weather\",\n",
      "    \"arguments\": {\n",
      "        \"city\": \"London\"\n",
      "    }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"model\": \"Trelis/Meta-Llama-3-8B-Instruct-function-calling\", # placholder, needed\n",
    "    \"top_p\": 0.6,\n",
    "    \"temperature\": 0.9,\n",
    "    \"max_tokens\": 512,\n",
    "    \"stop\": [\"<|eot_id|>\"],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "chat = llm.predict({\"messages\" :messages, **parameters})\n",
    "\n",
    "print(chat[\"choices\"][0][\"message\"][\"content\"].strip())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clean up\n",
    "\n",
    "To clean up, we can delete the model and endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.delete_model()\n",
    "llm.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "5fcf248a74081676ead7e77f54b2c239ba2921b952f7cbcdbbe5427323165924"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
