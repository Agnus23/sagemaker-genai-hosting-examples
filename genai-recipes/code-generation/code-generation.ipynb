{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78dc97c",
   "metadata": {},
   "source": [
    "# Fine tune and host your model for Code Generation in Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e567a4",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "Publicly available code LLMs such as Code Llama are great at generating code that adheres to general programming language principles and syntax, but they may not align with an organization’s internal conventions, or be aware of their internal proprietary libraries, coding standard/rubric and design best practices. In this notebook, we’ll see show how you can fine-tune a code LLM on private code bases to enhance its contextual awareness and improve a model’s usefulness to your organization’s needs. \n",
    "\n",
    "\n",
    "\n",
    "# Solution overview and approach\n",
    "\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to fine-tune Code LIama, deploy, and evaluate for code generation\n",
    "## Model details\n",
    "\n",
    "Code Llama is a code-specialized version of Llama 2 that was created by further training Llama 2 on its code-specific datasets and sampling more data from that same dataset for longer. Code Llama features enhanced coding capabilities. It can generate code and natural language about code, from both code and natural language prompts (for example, “Write me a function that outputs the Fibonacci sequence”). You can also use it for code completion and debugging. It supports many of the most popular programming languages used today, including Python, C++, Java, PHP, Typescript (JavaScript), C#, Bash, and more.\n",
    "Details about the Code Llama model can be found [here](https://huggingface.co/codellama)\n",
    "\n",
    "Meta published Code Llama [performance benchmarks](https://ai.meta.com/research/publications/code-llama-open-foundation-models-for-code/) on HumanEval and MBPP for common coding languages such as Python, Java, and JavaScript. The performance of Code Llama Python models on HumanEval demonstrated varying performance across different coding languages and tasks ranging from 38% on 7B Python model to 57% on 70B Python models. In addition, fine-tuned Code Llama models on SQL programming language have shown better results, as evident in SQL evaluation benchmarks. These published benchmarks highlight the potential benefits of fine-tuning Code Llama models, enabling better performance, customization, and adaptation to specific coding domains and tasks.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "We will be using Codellama model as a baseline and finetune it for [dolphin-coder dataset](https://huggingface.co/datasets/cognitivecomputations/dolphin-coder)\n",
    "\n",
    "The dataset format required for the instruction fine-tuning. The training data should be formatted in a JSON lines (.jsonl) format, where each line is a dictionary representing a data sample. Each sample in the JSON lines files must include system_prompt, question, and response fields.\n",
    "\n",
    "## Fine-tuning techniques \n",
    "Language models such as Llama are more than 10 GB or even 100 GB in size. Fine-tuning such large models requires instances with significantly high CUDA memory. Furthermore, training these models can be very slow due to the size of the model. Therefore, for efficient fine-tuning, we use the following optimizations:\n",
    "\n",
    "* Low-Rank Adaptation (LoRA)\n",
    "This is a type of parameter efficient fine-tuning (PEFT) for efficient fine-tuning of large models. With this method, you freeze the whole model and only add a small set of adjustable parameters or layers into the model. For instance, instead of training all 7 billion parameters for Llama 2 7B, you can fine-tune less than 1% of the parameters. This helps in significant reduction of the memory requirement because you only need to store gradients, optimizer states, and other training-related information for only 1% of the parameters. Furthermore, this helps in reduction of training time as well as the cost. \n",
    "\n",
    "* Int8 quantization \n",
    "Even with optimizations such as LoRA, models such as Llama 70B are still too big to train. To decrease the memory footprint during training, you can use Int8 quantization during training. Quantization typically reduces the precision of floating point data types. Although this decreases the memory required to store model weights, it degrades the performance due to loss of information. Int8 quantization uses only a quarter precision but doesn’t incur degradation of performance because it doesn’t simply drop the bits. It rounds the data from one type to the another. \n",
    "\n",
    "\n",
    "* Fully Sharded Data Parallel (FSDP) \n",
    "This is a type of data-parallel training algorithm that shards the model’s parameters across data parallel workers and can optionally offload part of the training computation to the CPUs. Although the parameters are sharded across different GPUs, computation of each microbatch is local to the GPU worker. It shards parameters more uniformly and achieves optimized performance via communication and computation overlapping during training.\n",
    "\n",
    "We will be finetuning the Codellama model from Sagemaker Jumpstart using LoRA techniques. \n",
    "\n",
    "\n",
    "## Notebook Walkthrough section\n",
    "\n",
    "### High level architecture\n",
    "\n",
    "![arch](./images/codellama.png)\n",
    "\n",
    "We will be walking through two different ways to host codellama models in Sagemaker. \n",
    "\n",
    "### Option 1 - Using model from hugging face etc. and host it on Sagemaker\n",
    "\n",
    "![huggingface](./images/huggingface.png)\n",
    "\n",
    "### Option 2 - Using Sagemaker Jumpstart\n",
    "\n",
    "![jumpstart](./images/jumpstart.png)\n",
    "\n",
    "The notebook requires users to specify following variables to start with.\n",
    "* Specify `model_id` (default value: `meta-textgeneration-llama-codellama-7b`)\n",
    "* Specify `accept_eula` argument to be True in `model.deploy()` to accept the end-user license agreement (EULA) before deployment the model in an endpoint, given Code LIama model is gated.\n",
    "* Sepcify `\"accept_eula\": \"true\"` in argument `environment` to accept the end-user license agreement (EULA) before fine-tuning.\n",
    "\n",
    "We will also see evaluation and comparison.\n",
    "\n",
    "So, let us get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af77efb",
   "metadata": {},
   "source": [
    "## 1. Pre-req Setup\n",
    "First, upgrade to the latest sagemaker SDK to ensure all available models are deployable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05b931-992e-4526-978d-f03196874a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet --upgrade sagemaker jmespath datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3959c936-0eca-4df9-9680-ebcd734a6b0e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 1: Import the model from hugging face and host it on AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60642984-bf52-44dd-8fcc-49bcb272022b",
   "metadata": {},
   "source": [
    "## 1. Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08176e87-094f-442b-ba88-90968256fd2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfe18c1-cb1c-42a8-b83b-3ac135974774",
   "metadata": {},
   "source": [
    "## Select the appropriate configuration parameters and container\n",
    "To optimize the deployment of Large Language Models (LLMs); one needs to choose the appropriate model partitioning framework, optimal batching technique, batching size, tensor parallelism degree, etc. The choice of a particular configuration depends on the usecase.\n",
    "\n",
    "Hence, based on the usecase, you need to:\n",
    "1. set the configuration parameters for the container.\n",
    "2. select the appropriate container image to be used for inference.\n",
    "\n",
    "### Set the configuration parameters using environment variables\n",
    "1. `SERVING_LOAD_MODELS` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the **MPI**. **MPI** is an engine that allows the model server to start distributed processes to load and serve the model.\n",
    "\n",
    "2. `OPTION_MODEL_ID`: Set this to the URI of the Amazon S3 bucket that contains the model. When this is set, the container leverages [s5cmd](https://github.com/peak/s5cmd) to download the model from s3. This enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance.\n",
    "If you want to download the model from huggingface.co, you can set `OPTION_MODEL_ID` to the model id of a pre-trained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co.\n",
    "\n",
    "3. `OPTION_TENSOR_PARALLEL_DEGREE`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the number of workers per model which will be started up when DJL serving runs. In this example we use the `ml.g5.4xlarge` instance that has 1 GPU; this is set to `max` to utilize all the GPUs on the instance.\n",
    "\n",
    "4. `OPTION_ROLLING_BATCH`: This parameter enables the use of a particular batching technique for continuous or iteration level batching to enable merging multiple concurrent requests that arrive at different times for inference. [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) is a TensorRT Toolbox for Optimized Large Language Model Inference on Nvidia GPUs. To leverage this, we set this parameter to `trtllm`.\n",
    "\n",
    "5. `OPTION_MAX_ROLLING_BATCH_SIZE`: The maximum number of concurrent requests to be used in a batch by the model server for inference. Clients can still send more requests to the endpoint, they will be queued.\n",
    "\n",
    "\n",
    "For more information on the available options, please refer to the [DJL Serving - SageMaker Large Model Inference Configurations](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212e91d4-0146-4cc6-be89-e8bddc4d3e44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_trtllm = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "              \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "              \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "              \"OPTION_MODEL_ID\": \"codellama/CodeLlama-7b-hf\",\n",
    "              \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "              \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "              \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "              \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "              \"OPTION_DTYPE\":\"fp16\"\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84023d6a-9bb7-485d-85a6-2d08774ab40c",
   "metadata": {
    "tags": []
   },
   "source": [
    "We leverage the tensorRT container; for other containers refer [Large Model Inference available DLC](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb51a0ba-7998-4785-b140-72d96adf2e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trtllm_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-tensorrtllm\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.26.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22204ec9-66e1-474c-94c8-aec979ae36c9",
   "metadata": {},
   "source": [
    "### When generating a large number of output tokens (> 1024), use the following configuration\n",
    "\n",
    "For more information on the available options, please refer to the [DJL Serving - SageMaker Large Model Inference Configurations](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c042b42e-d6e3-44e0-9c3e-e00a16c0078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_lmidist = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "               \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "               \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "               \"OPTION_MODEL_ID\": \"codellama/CodeLlama-7b-hf\",\n",
    "               \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "               \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "               \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "               \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "               \"OPTION_DTYPE\":\"fp16\"\n",
    "              }\n",
    "\n",
    "deepspeed_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", \n",
    "    region=sess.boto_session.region_name, \n",
    "    version=\"0.26.0\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868c88a7-6718-4c46-a09c-e002228367f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# - Select the appropriate environment variable which will tune the deployment server.\n",
    "#env = env_trtllm\n",
    "env = env_lmidist # use this when generating tokens > 1024  \n",
    "\n",
    "# - now we select the appropriate container \n",
    "inference_image_uri = deepspeed_image_uri # use this when generating tokens > 1024 \n",
    "#inference_image_uri = trtllm_image_uri\n",
    "\n",
    "print(f\"Environment variables are ---- > {env}\")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218f46b4-e428-4c37-8c9f-b702c76d1bc0",
   "metadata": {},
   "source": [
    "## 2. Host the model on Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2674337-d943-405d-ae62-5d081e4cd429",
   "metadata": {
    "tags": []
   },
   "source": [
    "To create the end point the steps are:\n",
    "1. Create the Model using the inference image container\n",
    "\n",
    "2. Create the endpoint config using the following key parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b48d2b-a7cf-469b-8d2d-0ad11de44385",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the model\n",
    "model_name = sagemaker.utils.name_from_base(\"lmi-codellama-7b\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df9d853-52d0-4688-b502-a29ecefff532",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": \"ml.g5.4xlarge\",\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 2400,\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8207c456-93d7-4418-a6e3-366b6623c112",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4934a5ae-30a2-4620-910d-5f86a7efcb9b",
   "metadata": {},
   "source": [
    "#### This step can take ~15 mins or longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b249a8-ccc9-48b5-b38e-17a498653111",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e605d0e3-71eb-49c3-ac73-f30998f456ac",
   "metadata": {},
   "source": [
    "## 3. Test/Invoke the deployed LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5cb2c9-447b-47da-b028-b1c1035e0ec1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"import socket \\n def ping_exponential_backoff(host: str):\"\"\"\n",
    "params = { \"max_new_tokens\":256, \n",
    "              \"temperature\":0.1}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps(\n",
    "        {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": params\n",
    "        }\n",
    "    ),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfc9beb-ca21-4139-b44e-2428741aca2f",
   "metadata": {},
   "source": [
    "## 4. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88621b0c-f44f-45a4-8a85-0ee381a9c341",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bc3a91-e5ee-4e69-8666-93b0c0b3e656",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Option 2 - Using Sagemaker Jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a0388",
   "metadata": {},
   "source": [
    "## 2. Deploy model\n",
    "\n",
    "Create a `JumpStartModel` object, which initializes default model configurations conditioned on the selected instance type. JumpStart already sets a default instance type, but you can deploy the model on other instance types by passing `instance_type` to the `JumpStartModel` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a2a8e5-789f-4041-9927-221257126653",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model_id=\"meta-textgeneration-llama-codellama-7b\"\n",
    "model_version = \"*\"\n",
    "\n",
    "model = JumpStartModel(model_id=model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259ad4f",
   "metadata": {},
   "source": [
    "You can now deploy the vanilla model using SageMaker JumpStart. If the selected model is gated, you will need to accept the end-user license agreement (EULA) prior to deployment. This is accomplished by providing the `accept_eula=True` argument to the `deploy` method. The deployment might take few minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3b42ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = model.deploy(\n",
    "    accept_eula=True\n",
    ")  # please change `accept_eula` to be True to accept EULA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728120a-0f1f-41d1-998b-172ce1f4e594",
   "metadata": {},
   "source": [
    "JumpStart stores model-specific default example payloads in its SDK. You can retrieve and view them using following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a68f1a-4520-4fef-8067-65c5041bf1eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictor.delete_predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10cf06c",
   "metadata": {},
   "source": [
    "### Invoke the endpoint\n",
    "\n",
    "This section demonstrates how to invoke the endpoint using example payloads that are retrieved programmatically from the `JumpStartModel` object. You can replace these example payloads with your own payloads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb364d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_payloads = model.retrieve_all_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5899c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jmespath\n",
    "\n",
    "\n",
    "for payload in example_payloads:\n",
    "    response = predictor.predict(payload.body)\n",
    "    generated_text = jmespath.search(payload.raw_payload[\"output_keys\"][\"generated_text\"], response)\n",
    "    print(\"Input:\\n\", payload.body[payload.prompt_key])\n",
    "    print(\"Output:\\n\", generated_text.strip())\n",
    "    print(\"\\n===============\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d44de0-67c0-4cc6-939f-ee2cd3ef717a",
   "metadata": {},
   "source": [
    "## 3. Fine-tune model with LoRA\n",
    "\n",
    "### Dataset preparation for instruction fine-tuning\n",
    "\n",
    "The training data must be formatted in a JSON lines (`.jsonl`) format, where each line is a dictionary representing a single data sample. All training data must be in a single folder, however it can be saved in multiple jsonl files. The `.jsonl` file extension is mandatory. The training\n",
    "folder can also contain a `template.json` file describing the input and output formats. If no template file is given, the following template will be used:\n",
    "  ```json\n",
    "  {\n",
    "    \"prompt\": \"{prompt}\",\n",
    "    \"completion\": \"{completion}\"\n",
    "  }\n",
    "  ```\n",
    "\n",
    "In this case, the data in the JSON lines entries must include `prompt` and `completion` fields. If a custom template is provided it must also use `prompt` and `completion` keys to define the input and output templates. Below is a sample custom template:\n",
    "  \n",
    "  ```json\n",
    "{\n",
    "    \"prompt\": \"{system_prompt} \\n\\n### Input: {question}\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "  ```\n",
    "Here, each example in the JSON lines must include `system_prompt`, `question` and `response` fields.\n",
    "\n",
    "In this demo, we will use a subset of [Dolphin-coder dataset](https://huggingface.co/datasets/cognitivecomputations/dolphin-coder) in an instruction tuning format. The dataset is available under Apache 2.0 license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58414e0d-4b8a-43ba-ae4e-6914758fec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "\n",
    "dolphin = load_dataset(\"cognitivecomputations/dolphin-coder\", split=\"train\")\n",
    "\n",
    "# We split the dataset into two where test data is used to evaluate at the end.\n",
    "train_and_test_dataset = dolphin.train_test_split(test_size=0.9, seed=0)\n",
    "\n",
    "# Dumping the training data to a local file to be used for training.\n",
    "train_and_test_dataset[\"train\"].to_json(\"train.jsonl\")\n",
    "train_and_test_dataset[\"test\"].select(range(10)).to_json(\"test.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d165680d-77b8-4563-80ce-f232db9c04cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_test_dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4bd800-a9b8-4ffb-a043-267ac63afa26",
   "metadata": {},
   "source": [
    "Next, we prepare prompt template used for processing the data in an instruction format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d30585-5972-488f-a4e6-63b7177a523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "template = {\n",
    "    \"prompt\": \"\"\"{system_prompt}\n",
    "\n",
    "### Input:\n",
    "{question}\n",
    "\"\"\",\n",
    "    \"completion\": \" {response}\",\n",
    "}\n",
    "with open(\"template.json\", \"w\") as f:\n",
    "    json.dump(template, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f414ec2e-adf3-449f-af36-9f2ce649c106",
   "metadata": {},
   "source": [
    "### Upload dataset to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729440e7-6a9c-4234-851e-0487f5f7b107",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.s3 import S3Uploader\n",
    "import sagemaker\n",
    "import random\n",
    "\n",
    "output_bucket = sagemaker.Session().default_bucket()\n",
    "local_data_file = \"train.jsonl\"\n",
    "train_data_location = f\"s3://{output_bucket}/dolphin_coder_dataset\"\n",
    "S3Uploader.upload(local_data_file, train_data_location)\n",
    "S3Uploader.upload(\"template.json\", train_data_location)\n",
    "print(f\"Training data: {train_data_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd38cde-f6f9-48e4-afa4-991772339331",
   "metadata": {},
   "source": [
    "Retrieve and customize hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db89ec-a1c7-4b1c-a64b-f348c3f193cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2829738-eca5-4bc9-944a-83f35ce55470",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_hyperparameters[\"epoch\"] = \"1\"\n",
    "print(my_hyperparameters)\n",
    "\n",
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4774e360-ed5e-4b59-b648-79312575fa49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.jumpstart.estimator import JumpStartEstimator\n",
    "\n",
    "\n",
    "estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    environment={\n",
    "        \"accept_eula\": \"true\"\n",
    "    },  # please change `accept_eula` to be `true` to accept EULA.\n",
    ")\n",
    "\n",
    "estimator.fit({\"training\": train_data_location})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081e71de-0eb8-44f4-ad47-82fdb0730662",
   "metadata": {},
   "source": [
    "### Deploy the fine-tuned model\n",
    "Next, we deploy the fine-tuned model. We will compare the performance of fine-tuned and pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419ba339-bbea-43f2-baec-0654ba1fdf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "finetuned_predictor = estimator.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f60eb4-068d-4173-ab97-cbc7b0d9c759",
   "metadata": {},
   "source": [
    "# Evaluation and comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707f93c8-39bf-4f0e-b589-d930e8e58de8",
   "metadata": {},
   "source": [
    "## Qualitatively evaluate the pre-trained and fine-tuned model\n",
    "Next, we use the test data to evaluate the performance of the fine-tuned model and compare it with the pre-trained model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93236ac6-884a-4eee-87eb-50df164b800c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "test_dataset = load_dataset(\"json\", data_files=\"test.jsonl\")[\"train\"]\n",
    "prompt_inference = template[\"prompt\"]\n",
    "inputs, ground_truth_responses, responses_before_finetuning, responses_after_finetuning = (\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    "    [],\n",
    ")\n",
    "\n",
    "\n",
    "def predict_and_print(datapoint):\n",
    "    # For instruction fine-tuning, we insert a special key between input and output\n",
    "    input_output_demarkation_key = \"\\n\\n### Response:\\n\"\n",
    "\n",
    "    payload = {\n",
    "        \"inputs\": prompt_inference.format(\n",
    "            system_prompt=datapoint[\"system_prompt\"], question=datapoint[\"question\"]\n",
    "        )\n",
    "        + input_output_demarkation_key,\n",
    "        \"parameters\": {\"max_new_tokens\": 100},\n",
    "    }\n",
    "    inputs.append(payload[\"inputs\"])\n",
    "    ground_truth_responses.append(datapoint[\"response\"])\n",
    "    pretrained_response = predictor.predict(payload)\n",
    "    responses_before_finetuning.append(pretrained_response[0][\"generated_text\"])\n",
    "    finetuned_response = finetuned_predictor.predict(payload)\n",
    "    responses_after_finetuning.append(finetuned_response[0][\"generated_text\"])\n",
    "\n",
    "\n",
    "try:\n",
    "    for i, datapoint in enumerate(test_dataset.select(range(5))):\n",
    "        predict_and_print(datapoint)\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {\n",
    "            \"Inputs\": inputs,\n",
    "            \"Ground Truth\": ground_truth_responses,\n",
    "            \"Response from non-finetuned model\": responses_before_finetuning,\n",
    "            \"Response from fine-tuned model\": responses_after_finetuning,\n",
    "        }\n",
    "    )\n",
    "    display(HTML(df.to_html()))\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b5ab2-9c58-44bb-a993-925dde778df9",
   "metadata": {},
   "source": [
    "## 4.2 Quantitatively evaluate the pre-trained and fine-tuned models using [Human-Eval repository](https://github.com/openai/human-eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177cf805",
   "metadata": {},
   "source": [
    "Lets now evaluate if our model has improved on the HumanEval metric from OpenAI. HumanEval is a standard benchmark for code generation models that was created using hand written python problems. This version of HumanEval is using python for its language of choice. We will generate solutions to 164 python related questions and then run a test suite on the solutions to generate a score. If you want to read more [here is the official paper.](https://arxiv.org/abs/2107.03374)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9122407-aaf6-4184-9e34-5fb2d05afcae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip3 install human_eval --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97065e3e-8fcc-48c1-8192-7778d312a281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from human_eval.evaluation import evaluate_functional_correctness\n",
    "from human_eval.data import write_jsonl, read_problems\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def generate_one_completion(prompt, predictor):\n",
    "    body = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\": 384, \"temperature\": 0.2}}\n",
    "\n",
    "    response = predictor.predict(body)\n",
    "\n",
    "    completion = (response[0][\"generated_text\"]).replace(prompt, \"\").split(\"\\n\\n\\n\")[0]\n",
    "    # if prompt is returned from response\n",
    "    completion = completion.replace(\"```\", \"\")\n",
    "    # if markdown code block is created\n",
    "    print(f\"payload: {prompt}\")\n",
    "    print(f\"completion: {completion}\")\n",
    "    return completion\n",
    "\n",
    "\n",
    "# perform HumanEval\n",
    "problems = read_problems()\n",
    "\n",
    "num_samples_per_task = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad620e6-4a06-4c12-8a7a-90ac84c1bfdc",
   "metadata": {},
   "source": [
    "Generate responses from pre-trained and fine-tuned models for 164 python related questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9de4da2-f359-4bdb-8a7c-159c86747411",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    dict(\n",
    "        task_id=task_id, completion=generate_one_completion(problems[task_id][\"prompt\"], predictor)\n",
    "    )\n",
    "    for task_id in tqdm(problems)\n",
    "    for _ in range(num_samples_per_task)\n",
    "]\n",
    "write_jsonl(\"pretrained.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67f656-2790-4031-811b-e1c738153c6c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "evaluate_functional_correctness(\"./pretrained.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e47914",
   "metadata": {},
   "source": [
    "Now lets compare the previous pretrained model to our new fine-tuned model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3444f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = [\n",
    "    dict(\n",
    "        task_id=task_id,\n",
    "        completion=generate_one_completion(problems[task_id][\"prompt\"], finetuned_predictor),\n",
    "    )\n",
    "    for task_id in tqdm(problems)\n",
    "    for _ in range(num_samples_per_task)\n",
    "]\n",
    "write_jsonl(\"fine-tuned.jsonl\", samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c75c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_functional_correctness(\"./fine-tuned.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ceef70b-23c4-4040-a508-640f445726af",
   "metadata": {},
   "source": [
    "### Clean up the endpoint\n",
    "Don't forget to clean up resources when finished to avoid unnecessary charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2d2f0-9971-4634-8273-1ec556085412",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_predictor()\n",
    "finetuned_predictor.delete_predictor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4998b86",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook, we saw how you can finetune and host LLM for code generation in Amazon Sagemaker. You can iterate further with your own dataset, different LLMs and different finetuning techniques and hyperparameters. "
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
