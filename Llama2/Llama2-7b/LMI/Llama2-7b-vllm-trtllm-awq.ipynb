{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eba763b2-d0f2-4e99-b375-8b4873116bcf",
   "metadata": {},
   "source": [
    "# Deploy Llama2-7b on Amazon SageMaker using LMI container\n",
    "\n",
    "## Resources\n",
    "- [Deep Learning Containers](https://docs.aws.amazon.com/sagemaker/latest/dg/large-model-inference-dlc.html)\n",
    "- [Deep Java Library - Large Model Inference](https://docs.djl.ai/docs/serving/serving/docs/large_model_inference.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944fbc1a-b23a-43c5-af27-76fb7fdaa25f",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dd04a8-6181-441e-b557-ee996281e3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4702f11-0670-4d8b-9c62-f325d77be6f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd76e135-46bb-4d47-a546-5de73abb5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")  # client to intreract with SageMaker Endpoints\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")\n",
    "print(f\"boto3 version: {boto3.__version__}\")\n",
    "print(f\"sagemaker version: {sagemaker.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a74f0e5-966c-4620-8bb9-a33e84c9ceeb",
   "metadata": {},
   "source": [
    "## Step 2: Endpoint Deployment (LMI - vLLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8b2c21-5edc-445d-938f-634e704c3173",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"0.27.0\"\n",
    "deepspeed_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region = region, version = version\n",
    ")\n",
    "print(f\"DeepSpeed image for vLLM is ----> {deepspeed_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f4cab6-c228-4fa3-a7d0-67feadcff891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# vLLM with DeepSpeed \n",
    "#\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_name = \"Llama-2-7b-chat-hf-vLLM\"\n",
    "\n",
    "# vLLM config\n",
    "vllm_config = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7B-Chat-fp16\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"1024\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"2048\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"2048\",\n",
    "    \"OPTION_DTYPE\": \"fp16\",\n",
    "}\n",
    "\n",
    "image_uri = deepspeed_image\n",
    "env = vllm_config\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e451d37b-39e3-44df-86cd-f425b33fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-EP-config\"\n",
    "health_check_timeout = 600\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": health_check_timeout,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9c1edd-768a-4fa5-abad-91807651512e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# Create endpoint config\n",
    "#\n",
    "endpoint_name = f\"{model_name}-EP\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c820a22a-29e4-4e9b-af26-ff100ea4878a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using helper function to wait for the endpoint to be ready\n",
    "#\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe50cfa-65fa-4fa3-b05b-0c1465a9ca09",
   "metadata": {},
   "source": [
    "## Step 3: Run Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64184c88-ad55-461b-9509-752eaee1a2f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#\n",
    "# define payload\n",
    "#\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    Body = json.dumps(payload),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "assistant = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a56eac-1251-49c0-bbed-21a40367f767",
   "metadata": {},
   "source": [
    "## Step 3.2: Test inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9724a55d-e6cc-4b4f-997c-723d660c7e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"P95: \" + str(np.percentile(results, 95)) + \" ms\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc4255ad-e07d-451a-945c-4b79889e8189",
   "metadata": {},
   "source": [
    "## Step 4: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04cd1d9-d71f-409f-b6a4-d9a9624d6432",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daaa34f-b756-42f0-b64b-24c17a68696c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d0ba606-81af-4e18-9ac6-c95354544fc2",
   "metadata": {},
   "source": [
    "## Step 5: Endpoint Deployment (LMI - TensorRT-LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bcdc20-4c51-4818-ad0b-5e8b317121d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"0.27.0\"\n",
    "trtllm_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-tensorrtllm\", region=region, version=version\n",
    ")\n",
    "print(f\"TensorRT-LLM image is ----> {trtllm_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b887a7-2b7a-421f-a0c1-fa21e40cec8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "instance_type = \"ml.g5.16xlarge\" # required for TensorRT-LLM Just In Time Compilation\n",
    "model_name = \"Llama-2-7b-chat-hf-TRTLLM\"\n",
    "\n",
    "trtllm_config = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7B-Chat-fp16\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"trtllm\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"1024\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"2048\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\"\n",
    "}\n",
    "\n",
    "image_uri = trtllm_image\n",
    "env = trtllm_config\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7303eff-9c27-4bae-81fc-5e74ed374f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-EP-config\"\n",
    "health_check_timeout = 1200\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": health_check_timeout,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a25cefc-f462-4aa0-a8a2-4bbcba1cf5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create endpoint config\n",
    "#\n",
    "endpoint_name = f\"{model_name}-EP\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def575da-0fda-44e8-bbf3-b04189720437",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using helper function to wait for the endpoint to be ready\n",
    "#\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfb9a21-111a-4b30-85b0-193599e567ec",
   "metadata": {},
   "source": [
    "## Step 6: Run Inference (TensorRT-LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60758b22-b3a8-43e8-8d6f-ef4b60153e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# define payload\n",
    "#\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    Body = json.dumps(payload),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "assistant = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0111de06-7e35-4082-9b19-4b23883685c6",
   "metadata": {},
   "source": [
    "## Step 6.2: Test inference performance (TensorRT-LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec9b986-f629-4e61-9fc2-7955f9229b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"P95: \" + str(np.percentile(results, 95)) + \" ms\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204f45fb-6f57-41da-9e84-292bc9e7993e",
   "metadata": {},
   "source": [
    "## Step 7: Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150bc35e-6a7c-4f0f-9bad-42a0517dea0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f4d74-1d51-4956-a5c7-b14d369ec624",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "677fff6c-2c3b-4a83-a479-21e886cc537a",
   "metadata": {},
   "source": [
    "## Step 8. Configure quantized Llama2-7b-chat-hf model using vLLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8486929c-57ed-4d08-82b8-c4bf685fc654",
   "metadata": {},
   "outputs": [],
   "source": [
    "version = \"0.27.0\"\n",
    "deepspeed_image = sagemaker.image_uris.retrieve(\n",
    "    \"djl-deepspeed\", region=region, version=version\n",
    ")\n",
    "print(f\"DeepSpeed image with vLLM is ----> {deepspeed_image}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115a0b19-a671-460f-8e09-7d6640c9d2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# vLLM with DeepSpeed \n",
    "#\n",
    "\n",
    "instance_type = \"ml.g5.2xlarge\"\n",
    "model_name = \"Llama-2-7b-chat-hf-AWQ\"\n",
    "\n",
    "# vLLM config\n",
    "vllm_config = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "    \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7B-Chat-AWQ\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"2\",\n",
    "    \"OPTION_MAX_INPUT_LEN\": \"1024\",\n",
    "    \"OPTION_MAX_OUTPUT_LEN\": \"2048\",\n",
    "    \"OPTION_MAX_MODEL_LEN\": \"2048\",\n",
    "    \"OPTION_QUANTIZE\": \"awq\",\n",
    "    \"OPTION_DTYPE\": \"auto\",\n",
    "}\n",
    "\n",
    "image_uri = deepspeed_image\n",
    "env = vllm_config\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName = model_name,\n",
    "    ExecutionRoleArn = role,\n",
    "    PrimaryContainer = {\n",
    "        \"Image\": image_uri,\n",
    "        \"Environment\": env,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348d9a32-3097-40e1-8dea-6db2502286c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_config_name = f\"{model_name}-EP-config\"\n",
    "health_check_timeout = 1200\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName = endpoint_config_name,\n",
    "    ProductionVariants = [\n",
    "        {\n",
    "            \"VariantName\": \"variant1\",\n",
    "            \"ModelName\": model_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": 1,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": health_check_timeout,\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32f0543-edf1-42c7-99f4-70df918a9fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Create endpoint config\n",
    "#\n",
    "endpoint_name = f\"{model_name}-EP\"\n",
    "\n",
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName = endpoint_name, EndpointConfigName = endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e6fb04-023b-4701-a522-d122f30af43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Using helper function to wait for the endpoint to be ready\n",
    "#\n",
    "sess.wait_for_endpoint(endpoint_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb638f0-c082-440a-86e3-1bfe0b2a1e62",
   "metadata": {},
   "source": [
    "## Step 9: Inference (vLLM - AWQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5274c-41a5-440d-9c2d-fc0e5b855d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# define payload\n",
    "#\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    EndpointName = endpoint_name,\n",
    "    Body = json.dumps(payload),\n",
    "    ContentType = \"application/json\",\n",
    ")\n",
    "\n",
    "assistant = json.loads(response_model[\"Body\"].read().decode(\"utf8\"))[\"generated_text\"]\n",
    "print(assistant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1ae417-2905-4650-aa17-1210c0070a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "# Calculate runtime performance\n",
    "# \n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# define payload\n",
    "prompt = \"\"\"You are an helpful Assistant, called Jarvis. Knowing everyting about AWS.\n",
    "\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Jarvis:\"\"\"\n",
    "\n",
    "params = { \"max_new_tokens\": 256, \"temperature\": 0.1}\n",
    "\n",
    "# hyperparameters for llm (remove \"\\nUser:\" from stop conditions)\n",
    "payload = {\n",
    "  \"inputs\": prompt,\n",
    "  \"parameters\": params\n",
    "}\n",
    "\n",
    "results = []\n",
    "for i in range(0, 10):\n",
    "    start = time.time()\n",
    "    response_model = smr_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        Body=json.dumps(payload),\n",
    "        ContentType=\"application/json\",\n",
    "    )\n",
    "    results.append((time.time() - start) * 1000)\n",
    "\n",
    "print(\"\\nPredictions for model latency: \\n\")\n",
    "print(\"P95: \" + str(np.percentile(results, 95)) + \" ms\")\n",
    "print(\"P90: \" + str(np.percentile(results, 90)) + \" ms\")\n",
    "print(\"Average: \" + str(np.average(results)) + \" ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7428fd5-7239-4666-a078-f14009d21e30",
   "metadata": {},
   "source": [
    "## Step 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1092ca-ae76-4595-a1f5-5df95010a799",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_config_name)\n",
    "sess.delete_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209f9ad-8d05-4959-b67e-598766de1447",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
