{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a729a7fc-a1b6-4832-9155-d240ccd8ecc0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Serving LoRA-based Llama 2 and Mistral adapters with high performance on SageMaker \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2958bcf-767f-4cd7-826e-963f91565876",
   "metadata": {},
   "source": [
    "This notebook will demonstrate how you can deploy multiple base models and their fine-tuned LoRA adapters on SageMaker using the DJL Serving Large Model Inference DLC. LoRA (Low Rank Adapters) is a powerful technique for fine-tuning large language models. This technique significantly reduces the number of trainable parameters compared to traditional fine-tuning while achieving comparable or superior performance. You can learn more about the LoRA technique in this paper.\n",
    "\n",
    "A major benefit of LoRA is that the fine-tuned adapters can easily be added to and removed from the base model, which makes switching adapters pretty cheap and viable at runtime. In this notebook we will show how you can deploy a SageMaker endpoint with a single base model and multiple LoRA adapters, and change adapters for different requests.\n",
    "\n",
    "Since LoRA adapters are much smaller than the size of a base model (can realistically be 100x-1000x smaller), we can deploy an endpoint with a single base model and multiple LoRA adapters using much less hardware than deploying an equivalent number of fully fine-tuned models.\n",
    "\n",
    "In this notebook, we deploy the [llama2-7B](https://huggingface.co/TheBloke/Llama-2-7b-fp16) and the [mistral-7B](https://huggingface.co/mistralai/Mistral-7B-v0.1) as the base models and their LoRA adapters fine tuned for a specific language on the same SageMaker endpoint as shown below by leveraging the [SageMaker Large Model Inference Container](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers).\n",
    "\n",
    "![Multiple adapters for Llama 2 as a base model](adapter-basemodel.png)\n",
    "\n",
    "The LMI container offers the out-of-box integration with SageMaker for hosting multiple LoRA adapters with higher performance (low latency and high throughput) using the [vLLM](https://docs.vllm.ai/en/latest/models/lora.html) library that uses [S-LORA](https://github.com/S-LoRA/S-LoRA) and [Punica](https://arxiv.org/pdf/2310.18547.pdf). S-LoRA stores all adapters in the main memory and fetches the adapters used by the currently running queries to the GPU memory. To efficiently use the GPU memory and reduce fragmentation, S-LoRA proposes Unified Paging. Unified Paging uses a unified memory pool to manage dynamic adapter weights with different ranks and KV cache tensors with varying sequence lengths. Additionally, S-LoRA employs a novel tensor parallelism strategy and highly optimized custom CUDA kernels for heterogeneous batching of LoRA computation. Collectively, these features enable S-LoRA to serve thousands of LoRA adapters on a single GPU or across multiple GPUs with a small overhead.\n",
    "\n",
    "Below diagram shows the Multi LoRA-Adapter serving stack of LMI container on SageMaker\n",
    "![Multi LoRA-Adapter serving stack of LMI container on SageMaker](LoRA-LMI-SageMaker.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ffcfa8-02c5-4492-af5c-fdcf60f369db",
   "metadata": {},
   "source": [
    "# License agreement\n",
    " - View license information https://huggingface.co/meta-llama before using the model.\n",
    " - This notebook is a sample notebook and not intended for production use. Please refer to the licence at https://github.com/aws/mit-0. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a133db-f99f-4e46-8fe1-1a5af6a846a1",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Install, import the required libraries; set some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8dd6a36f-ee95-453d-a127-c8a7de6a026d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install sagemaker boto3 huggingface_hub awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf0c89f4-679c-4557-b95d-1d954c15a020",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import jinja2\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import snapshot_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "561b79ba-7354-4d40-87ce-dc5813092576",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f68b5181-d018-4564-9762-fa8770a9672f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_bucket = sess.default_bucket()  # bucket to house model artifacts\n",
    "s3_code_prefix = \"hf-large-model-djl/multi-lora/Llama-2-7b-fp16/code\"  # folder within bucket where model/code artifacts will go\n",
    "s3_code_prefix2 = \"hf-large-model-djl/multi-lora/Mistral-7b-fp16/code\"  # folder within bucket where model/code artifacts will go\n",
    "\n",
    "region = sess._region_name\n",
    "account_id = sess.account_id()\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "sm_client = boto3.client(\"sagemaker\")\n",
    "smr_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "jinja_env = jinja2.Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c28a75-9e5c-4be7-8ac2-f3c9f69cbb10",
   "metadata": {},
   "source": [
    "We will be deploying an endpoint with 3 LoRA adapters. These are the models we will be using:\n",
    "\n",
    "Base Model: https://huggingface.co/TheBloke/Llama-2-7B-Chat-fp16\n",
    "LoRA Fine Tuned Adapter 1: https://huggingface.co/UnderstandLing/llama-2-7b-chat-ru\n",
    "LoRA Fine Tuned Adapter 2: https://huggingface.co/UnderstandLing/llama-2-7b-chat-es\n",
    "LoRA Fine Tuned Adapter 3: https://huggingface.co/UnderstandLing/llama-2-7b-chat-fr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558d3f19-173e-460c-9be3-e54a7dcf0b0c",
   "metadata": {
    "tags": []
   },
   "source": [
    "The core structure to cover here is the model directory. We include both the base model and LoRA adapters in the model directory like this:\n",
    "\n",
    "```\n",
    "|- model_dir\n",
    "    |- adapters/\n",
    "        |--- <adapter_1>/\n",
    "        |--- <adapter_2>/\n",
    "        |--- ...\n",
    "        |--- <adapter_n>/\n",
    "```\n",
    "\n",
    "It is also possible to have model files located in a separate s3 bucket by specifying that location using an s3 `option.model_id` in the serving.properties. In this case, the adapters directory can be located either alongside the `serving.properties` or alongside the model files in s3.\n",
    "\n",
    "Each of the adapters in the `adapters` directory contains the LoRA adapter artifacts. Typically there are two files: `adapter_model.bin` and `adapter_config.json` which are the adapter weights and adapter configuration respectively. These are typically obtained from the Peft library via the `PeftModel.save_pretrained()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb64850-08a5-4052-b5ac-e150cd60b54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf llama-lora-multi-adapter\n",
    "!mkdir -p llama-lora-multi-adapter/adapters\n",
    "!echo \"Lora Multi Adapter Model\" > llama-lora-multi-adapter/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d8c8fc-1640-4f07-899a-e71667a7173f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# snapshot_download(\"UnderstandLing/llama-2-7b-chat-ru\", local_dir=\"llama-lora-multi-adapter/adapters/ru\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fec6d2fa-d5be-41d9-ae0e-2432e24f8ea7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/huggingface_hub/file_download.py:1194: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.\n",
      "For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5b86ebc11144a9a9a7121922ac6d59c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dca541480744439ba3c1b6b5ea837466",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b95bbc8091d449a59cbbc4958bf3b42a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9021d6f462054a73a1ebbebe9f909898",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209dd2940af046bf84eb6e54b067bf11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee3f9a106eb84152bbddec982a9459a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6765179575c84e1f9e9320d3ae5b1993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d527e3903a014828b9291dfcbc77205c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd7a2d6ec6ab46749d7ca2f706941f32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69dc1bd9fe9444a5bd46afd8d7499424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples/LORA-adapters-IC/llama-lora-multi-adapter/adapters/es'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"UnderstandLing/llama-2-7b-chat-es\", local_dir=\"llama-lora-multi-adapter/adapters/es\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8d540e8-1873-485d-a46a-ac64778aeea8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f410e4972c7e41d3896bb885e0935264",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 9 files:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f08256f020c4f658a9867feef503a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20aa334d4b674eb7a7e373de16c35656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e9013ed38284b88b70b22cdcc23513c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/435 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3b905875874875a8e85c9a738fabcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/21.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01f4cfefbcda4c8c89dc0755d715d1c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/5.51k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709db03dfa894eaba475504a26047d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/498 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2444d15f964f9f96d33e9cec701233",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.safetensors:   0%|          | 0.00/134M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61d72e990c4942cf984280f6f3b1f544",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.06k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f3e73aa100d4fd2b5d8b07b26f8a73f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples/LORA-adapters-IC/llama-lora-multi-adapter/adapters/fr'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"UnderstandLing/llama-2-7b-chat-fr\", local_dir=\"llama-lora-multi-adapter/adapters/fr\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1e803e2-40af-4c49-b65c-ff3bc244d621",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !rm -f model.tar.gz\n",
    "# !rm -rf llama-lora-multi-adapter/.ipynb_checkpoints\n",
    "# !tar czvf model.tar.gz -C llama-lora-multi-adapter ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "255205e1-5df7-428e-a2ba-a1e53ddc73ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./adapters/\n",
      "./adapters/es/\n",
      "./adapters/es/.huggingface/\n",
      "./adapters/es/.huggingface/.gitignore\n",
      "./adapters/es/.huggingface/download/\n",
      "./adapters/es/.huggingface/download/.gitattributes.lock\n",
      "./adapters/es/.huggingface/download/README.md.lock\n",
      "./adapters/es/.huggingface/download/adapter_config.json.lock\n",
      "./adapters/es/.huggingface/download/adapter_model.safetensors.lock\n",
      "./adapters/es/.huggingface/download/added_tokens.json.lock\n",
      "./adapters/es/.huggingface/download/special_tokens_map.json.lock\n",
      "./adapters/es/.huggingface/download/tokenizer.json.lock\n",
      "./adapters/es/.huggingface/download/tokenizer.model.lock\n",
      "./adapters/es/.huggingface/download/README.md.metadata\n",
      "./adapters/es/.huggingface/download/tokenizer_config.json.lock\n",
      "./adapters/es/.huggingface/download/adapter_config.json.metadata\n",
      "./adapters/es/.huggingface/download/special_tokens_map.json.metadata\n",
      "./adapters/es/.huggingface/download/added_tokens.json.metadata\n",
      "./adapters/es/.huggingface/download/.gitattributes.metadata\n",
      "./adapters/es/.huggingface/download/tokenizer_config.json.metadata\n",
      "./adapters/es/.huggingface/download/tokenizer.json.metadata\n",
      "./adapters/es/.huggingface/download/tokenizer.model.metadata\n",
      "./adapters/es/.huggingface/download/adapter_model.safetensors.metadata\n",
      "./adapters/es/README.md\n",
      "./adapters/es/adapter_config.json\n",
      "./adapters/es/added_tokens.json\n",
      "./adapters/es/special_tokens_map.json\n",
      "./adapters/es/.gitattributes\n",
      "./adapters/es/tokenizer_config.json\n",
      "./adapters/es/tokenizer.json\n",
      "./adapters/es/tokenizer.model\n",
      "./adapters/es/adapter_model.safetensors\n",
      "./adapters/fr/\n",
      "./adapters/fr/.huggingface/\n",
      "./adapters/fr/.huggingface/.gitignore\n",
      "./adapters/fr/.huggingface/download/\n",
      "./adapters/fr/.huggingface/download/.gitattributes.lock\n",
      "./adapters/fr/.huggingface/download/README.md.lock\n",
      "./adapters/fr/.huggingface/download/adapter_config.json.lock\n",
      "./adapters/fr/.huggingface/download/adapter_model.safetensors.lock\n",
      "./adapters/fr/.huggingface/download/special_tokens_map.json.lock\n",
      "./adapters/fr/.huggingface/download/added_tokens.json.lock\n",
      "./adapters/fr/.huggingface/download/tokenizer.json.lock\n",
      "./adapters/fr/.huggingface/download/tokenizer.model.lock\n",
      "./adapters/fr/.huggingface/download/special_tokens_map.json.metadata\n",
      "./adapters/fr/.huggingface/download/.gitattributes.metadata\n",
      "./adapters/fr/.huggingface/download/tokenizer_config.json.lock\n",
      "./adapters/fr/.huggingface/download/added_tokens.json.metadata\n",
      "./adapters/fr/.huggingface/download/README.md.metadata\n",
      "./adapters/fr/.huggingface/download/adapter_config.json.metadata\n",
      "./adapters/fr/.huggingface/download/tokenizer.json.metadata\n",
      "./adapters/fr/.huggingface/download/tokenizer_config.json.metadata\n",
      "./adapters/fr/.huggingface/download/tokenizer.model.metadata\n",
      "./adapters/fr/.huggingface/download/adapter_model.safetensors.metadata\n",
      "./adapters/fr/.gitattributes\n",
      "./adapters/fr/special_tokens_map.json\n",
      "./adapters/fr/added_tokens.json\n",
      "./adapters/fr/README.md\n",
      "./adapters/fr/adapter_config.json\n",
      "./adapters/fr/tokenizer.json\n",
      "./adapters/fr/tokenizer_config.json\n",
      "./adapters/fr/tokenizer.model\n",
      "./adapters/fr/adapter_model.safetensors\n",
      "./README.txt\n"
     ]
    }
   ],
   "source": [
    "!rm -f adapters.tar.gz\n",
    "!rm -rf llama-lora-multi-adapter/.ipynb_checkpoints\n",
    "!tar czvf adapters.tar.gz -C llama-lora-multi-adapter ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13c1d4a-29b3-4f87-848f-a3854cee227d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_artifact_accelerate = sess.upload_data(\"adapters.tar.gz\", model_bucket, s3_code_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f8270-69e1-47a0-ad6e-076da0f09e28",
   "metadata": {},
   "source": [
    "### Select the appropriate configuration parameters and container¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d559783-8b78-4c24-8220-82e75c323d51",
   "metadata": {},
   "source": [
    "To optimize the deployment of Large Language Models (LLMs); one needs to choose the appropriate model partitioning framework, optimal batching technique, batching size, tensor parallelism degree, etc. The choice of a particular configuration depends on the usecase.\n",
    "\n",
    "Hence, based on the usecase, you need to:\n",
    "1. set the configuration parameters for the container.\n",
    "2. select the appropriate container image to be used for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11eddcc-d301-4acc-86b5-1612501aaa39",
   "metadata": {},
   "source": [
    "### Set the configuration parameters using environment variables\n",
    "1. `SERVING_LOAD_MODELS` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the **Python** engine.\n",
    "\n",
    "2. `OPTION_MODEL_ID`: Set this to the URI of the Amazon S3 bucket that contains the model. When this is set, the container leverages [s5cmd](https://github.com/peak/s5cmd) to download the model from S3. This enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance.\n",
    "If you want to download the model from huggingface.co, you can set `OPTION_MODEL_ID` to the model id of a pre-trained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). The container uses this model id to download the corresponding model repository on huggingface.co.\n",
    "\n",
    "3. `OPTION_TENSOR_PARALLEL_DEGREE`: Set to the number of GPU devices over which DeepSpeed needs to partition the model. This parameter also controls the number of workers per model which will be started up when DJL serving runs. In this example we use the `ml.g5.12xlarge` instance that has 4 GPUs; hence this is set to 4.\n",
    "\n",
    "4. `OPTION_ROLLING_BATCH`: This parameter enables the use of a particular batching technique for continuous or iteration level batching to enable merging multiple concurrent requests that arrive at different times for inference.\n",
    "In scenarios that involves open ended generation and chatbots, there is a need for having a high throughput. [vLLM](https://arxiv.org/pdf/2309.06180.pdf) is a fast LLM inference and serving framework that uses techniques like PagedAttention and continuous batching to improve the throughput. Hence, we set the `rolling_batch` parameter to `vllm`. When using `vllm`, you can also use some [additional parameters](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md#vllm).\n",
    "\n",
    "5. `OPTION_MAX_ROLLING_BATCH_SIZE`: The maximum number of concurrent requests to be used in a batch by the model server for inference. Clients can still send more requests to the endpoint, they will be queued.\n",
    "\n",
    "6. `OPTION_ENABLE_LORA`: This config enables support for LoRA adapters. Default: false.\n",
    "\n",
    "7. `OPTION_MAX_LORAS`: This config determines the maximum number of LoRA adapters that can be run at once. Allocates more GPU memory for those adapters. Default: 4\n",
    "\n",
    "8. `OPTION_MAX_LORA_RANK`: This config determines the maximum rank allowed for a LoRA adapter. Setting a larger value will enable more adapters at a greater memory usage cost. Default: 16\n",
    "\n",
    "9. `OPTION_LORA_EXTRA_VOCAD_SIZE`: This config determines the maximum additional vocabulary that can be added through a LoRA adapter. Default: 256\n",
    "\n",
    "10. `OPTION_MAX_CPU_LORAS`: This config determines the maximum number of LoRA adapters to cache in memory. All others will be evicted to disk. Default: None\n",
    "\n",
    "\n",
    "For more information on the available options, please refer to the [DJL Serving - SageMaker Large Model Inference Configurations](https://github.com/deepjavalibrary/djl-serving/blob/master/serving/docs/lmi/configurations_large_model_inference_containers.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c92745-edd2-4187-8b19-b05b0a3c5b5b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Select the relevant Large Model Inference container\n",
    "SageMaker offers optimized [large model inference containers](https://github.com/aws/deep-learning-containers/blob/master/available_images.md#large-model-inference-containers) that contains different frameworks for model parallelism enabling inference of LLMs on multiple GPUs.\n",
    "\n",
    "In this scenario, since we are leveraging `vllm` as the batching technique, we leverage the `deepspeed` container that has frameworks like deepspeed, vllm, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68716eed-94eb-4f70-88ea-92d15707bc8b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepspeed_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.27.0\"\n",
    ")\n",
    "\n",
    "env_generation = {\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "                  \"TRANSFORMERS_CACHE\": \"/tmp\",\n",
    "                  \"OPTION_MODEL_ID\": \"TheBloke/Llama-2-7B-Chat-fp16\",\n",
    "                  \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "                  \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "                  \"OPTION_ROLLING_BATCH\": \"lmi-dist\",\n",
    "                  \"OPTION_MAX_ROLLING_BATCH_SIZE\": \"32\",\n",
    "                  \"OPTION_DTYPE\": \"fp16\",\n",
    "                  \"OPTION_ENABLE_LORA\": \"true\",\n",
    "                  \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.8\",\n",
    "                  \"OPTION_MAX_LORA_RANK\": \"64\",\n",
    "                  \"OPTION_MAX_CPU_LORAS\": \"4\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858e2fbc-30aa-4b89-8c2f-8f1a0438c0f0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are ---- > {'HUGGINGFACE_HUB_CACHE': '/tmp', 'TRANSFORMERS_CACHE': '/tmp', 'OPTION_MODEL_ID': 'TheBloke/Llama-2-7B-Chat-fp16', 'OPTION_TRUST_REMOTE_CODE': 'true', 'OPTION_TENSOR_PARALLEL_DEGREE': '1', 'OPTION_ROLLING_BATCH': 'lmi-dist', 'OPTION_MAX_ROLLING_BATCH_SIZE': '32', 'OPTION_DTYPE': 'fp16', 'OPTION_ENABLE_LORA': 'true', 'OPTION_GPU_MEMORY_UTILIZATION': '0.8', 'OPTION_MAX_LORA_RANK': '64', 'OPTION_MAX_CPU_LORAS': '4'}\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "# - Select the appropriate environment variable which will tune the deployment server.\n",
    "env = env_generation # use this in case it is 'generation' task \n",
    "# - now we select the appropriate container \n",
    "inference_image_uri = deepspeed_image_uri # use this in case it is 'generation' task \n",
    "#inference_image_uri = trtllm_image_uri # enable this in case your use case is summarization ( high input and medium output sizes ) \n",
    "\n",
    "print(f\"Environment variables are ---- > {env}\")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287be57e-2765-4384-becd-b4e2825a1025",
   "metadata": {},
   "source": [
    "### Create an endpoint config\n",
    "Create an endpoint configuration using the appropriate instance type. Set the `ContainerStartupHealthCheckTimeoutInSeconds` to account for the time taken to download the LLM weights from S3 or the model hub; and the time taken to load the model on the GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d347b4d5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi-llama2-7b-2024-05-14-02-51-09-214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'EndpointConfigArn': 'arn:aws:sagemaker:us-east-1:972812897072:endpoint-config/lmi-llama2-7b-2024-05-14-02-51-09-214-config',\n",
       " 'ResponseMetadata': {'RequestId': 'fd5c58f0-1ccf-42ba-8afa-0d14a7b2b09f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': 'fd5c58f0-1ccf-42ba-8afa-0d14a7b2b09f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '125',\n",
       "   'date': 'Tue, 14 May 2024 02:51:09 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"lmi-llama2-7b\")\n",
    "print(model_name)\n",
    "\n",
    "endpoint_config_name = f\"{model_name}-config\"\n",
    "endpoint_name = f\"{model_name}-endpoint\"\n",
    "\n",
    "# Set varient name and instance type for hosting\n",
    "variant_name = \"AllTraffic\"\n",
    "instance_type = \"ml.g5.12xlarge\"\n",
    "model_data_download_timeout_in_seconds = 1600\n",
    "container_startup_health_check_timeout_in_seconds = 1600\n",
    "initial_instance_count = 1\n",
    "max_instance_count = 1\n",
    "\n",
    "endpoint_config_response = sm_client.create_endpoint_config(\n",
    "    EndpointConfigName=endpoint_config_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    ProductionVariants=[\n",
    "        {\n",
    "            \"VariantName\": variant_name,\n",
    "            \"InstanceType\": instance_type,\n",
    "            \"InitialInstanceCount\": initial_instance_count,\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": model_data_download_timeout_in_seconds,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": container_startup_health_check_timeout_in_seconds,\n",
    "            \"ManagedInstanceScaling\": {\n",
    "                \"Status\": \"ENABLED\",\n",
    "                \"MinInstanceCount\": initial_instance_count,\n",
    "                \"MaxInstanceCount\": max_instance_count,\n",
    "            },\n",
    "            \"RoutingConfig\": {\n",
    "                'RoutingStrategy': 'LEAST_OUTSTANDING_REQUESTS'\n",
    "            },\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "endpoint_config_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0534b2-aa1c-4cdd-9ac9-0f4466ee5b22",
   "metadata": {},
   "source": [
    "### Create an endpoint using the endpoint config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1150973-4e6c-4c8e-9d35-23fe50eb90e6",
   "metadata": {},
   "source": [
    "To create the end point the steps are:\n",
    "\n",
    "- Create the endpoint config using the following key parameters\n",
    "\n",
    "In this notebook we leverage the boto3 SDK. You can also use the [SageMaker SDK](https://sagemaker.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "caaee3e7-784a-42fb-bf5c-d2fb5eb113c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Endpoint: arn:aws:sagemaker:us-east-1:972812897072:endpoint/lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint\n"
     ]
    }
   ],
   "source": [
    "create_endpoint_response = sm_client.create_endpoint(\n",
    "    EndpointName=f\"{endpoint_name}\", EndpointConfigName=endpoint_config_name\n",
    ")\n",
    "print(f\"Created Endpoint: {create_endpoint_response['EndpointArn']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f03ce0a-ce37-44d8-82ec-833d2712f369",
   "metadata": {},
   "source": [
    "#### This step can take ~10 mins or longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1545e376-48e2-4b8b-ac83-cdb8fa8cd63d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n",
      "Arn: arn:aws:sagemaker:us-east-1:972812897072:endpoint/lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "status = resp[\"EndpointStatus\"]\n",
    "print(\"Status: \" + status)\n",
    "\n",
    "while status == \"Creating\":\n",
    "    time.sleep(60)\n",
    "    resp = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = resp[\"EndpointStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "\n",
    "print(\"Arn: \" + resp[\"EndpointArn\"])\n",
    "print(\"Status: \" + status)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2f4304-732d-48d2-b5eb-1b0dea2fc912",
   "metadata": {},
   "source": [
    "## Create an inference component to your endpoint for Llama-2-7B-chat with LoRA adapters\n",
    "Inference components can reuse a SageMaker model that you may have already created. You also have the option to specify your artifacts and container directly when creating an inference component which we will show below. In this example we will also create a SageMaker model if you want to reference it later. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8454457-36fd-44c4-a064-0443e97718a0",
   "metadata": {},
   "source": [
    "### Create the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9c057-3caf-41de-8bcf-d0ac91f3f86d",
   "metadata": {},
   "source": [
    "Create the Model\n",
    "Leverage the `inference_image_uri` to create a model object. We will leverage the Least routing algorithim -- [Least Routing Algorithim](https://aws.amazon.com/blogs/machine-learning/minimize-real-time-inference-latency-by-using-amazon-sagemaker-routing-strategies/). This innovation from sagemnaker has shown to reduce latency by 10% or more when we have multiple instances configured to serve the endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "784e2b93-783f-411b-9ff9-2b3436e4bb1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi-llama2-7b-2024-05-14-02-55-10-800\n",
      "Created Model: arn:aws:sagemaker:us-east-1:972812897072:model/lmi-llama2-7b-2024-05-14-02-55-10-800\n"
     ]
    }
   ],
   "source": [
    "model_name = sagemaker.utils.name_from_base(\"lmi-llama2-7b\")\n",
    "print(model_name)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"Environment\": env,\n",
    "        \"ModelDataUrl\": s3_code_artifact_accelerate,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec8283f-1e3e-4417-a1bd-e11963421c92",
   "metadata": {},
   "source": [
    "### Create Inference Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c185447-da6d-4428-a21c-c8d4448e1e1b",
   "metadata": {},
   "source": [
    "We can now create our inference component. Note below that we specify an inference component name. You can use this name to update your inference compent or view metrics and logs on the inference component you create in CloudWatch. You will also want to set your \"ComputeResourceRequirements\". This will tell SageMaker how much of each resource you want to reserver for EACH COPY of your inference component. Finally we set the number of copies that we want to deploy. The number of copies can be managed through autoscaling policies. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d015f240-1f5c-4ff1-a1b9-beb3d3f5e68f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo inference component name: lmi-llama2-7b-1715655311-a135-inference-component:: endpoint_name=lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint\n"
     ]
    }
   ],
   "source": [
    "prefix = sagemaker.utils.unique_name_from_base(\"lmi-llama2-7b\")\n",
    "\n",
    "inference_component_name = f\"{prefix}-inference-component\"\n",
    "print(f\"Demo inference component name: {inference_component_name}:: endpoint_name={endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e8ff281f-6890-4a09-8f05-ce40778e11db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceComponentArn': 'arn:aws:sagemaker:us-east-1:972812897072:inference-component/lmi-llama2-7b-1715655311-a135-inference-component',\n",
       " 'ResponseMetadata': {'RequestId': '24e5628c-9dd6-4e16-8845-32234174da65',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '24e5628c-9dd6-4e16-8845-32234174da65',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '138',\n",
       "   'date': 'Tue, 14 May 2024 02:55:11 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name,\n",
    "        # \"Container\": {\n",
    "        #     \"Image\": inference_image_uri,\n",
    "        #     \"ArtifactUrl\": s3_code_artifact,\n",
    "        # },\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 1200,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 7*2*1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a481217e-9fcc-428b-af5a-06495491588b",
   "metadata": {},
   "source": [
    "#### This step can take ~15 mins or longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c61d35a-a11a-4a42-b00b-c4c221daf929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "while True:\n",
    "    desc = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce7be01-f2c5-47fd-bf09-9454764d7e47",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "450e8379-9f4e-4d4a-904a-81ebfb6a1c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \"max_new_tokens\": 100}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "721e1b3b-4b61-456d-b992-a50aa16fb203",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.9 ms, sys: 2.83 ms, total: 18.7 ms\n",
      "Wall time: 3.55 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"generated_text\": \"\\\\n\\\\nEsta es una lista de excusas creativas para decir que no necesitas ir a la fiesta:\\\\n\\\\n1. Tengo que hacer una tarea urgente para la escuela.\\\\n2. Tengo que ir a la tienda a comprar algo importante.\\\\n3. Tengo que hacer una llamada importante para mi trabajo.\\\\n4. Tengo que hacer una llamada importante para mi trabajo.\\\\n5. Tengo que hacer una llam\"}'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Testing Spanish (es) adapter\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"Piensa en una excusa creativa para decir que no necesito ir a la fiesta.\"],\n",
    "                     \"parameters\": params,\n",
    "                     \"adapters\": [\"es\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4bdcf208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.46 ms, sys: 0 ns, total: 4.46 ms\n",
      "Wall time: 2.98 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\"generated_text\": \"\\\\n\\\\nPensez à une excuse créative pour dire que je n\\'ai pas besoin d\\'aller à la fête.\"}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# Testing French (fr) adapter\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    InferenceComponentName=inference_component_name,\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"Pensez à une excuse créative pour dire que je n'ai pas besoin d'aller à la fête.\"],\n",
    "                     \"parameters\": params,\n",
    "                     \"adapters\": [\"fr\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35d1b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "# Testing Russian (ru) adapter\n",
    "# response_model = smr_client.invoke_endpoint(\n",
    "#     InferenceComponentName=inference_component_name,\n",
    "#     EndpointName=endpoint_name,\n",
    "#     Body=json.dumps({\"inputs\": [\"Придумайте креативное оправдание, чтобы сказать, что мне не нужно идти на вечеринку.\"],\n",
    "#                      \"parameters\": params,\n",
    "#                      \"adapters\": [\"ru\"]}),\n",
    "#     ContentType=\"application/json\",\n",
    "# )\n",
    "\n",
    "# response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9b44c0-5486-434a-a5b4-bf02271c5567",
   "metadata": {},
   "source": [
    "## Mistral 7B with LoRA adapter\n",
    "To showcase multiple-base models with their LoRA adapters, we will add another base model, mistralai/Mistral-7B-v0.1, and it’s LoRA adapter to the same SageMaker endpoint. Here's we are basically repeating similar steps for the preivous Llama2 base model and its LoRA adapter for the Mistral 7B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5e191dbf-7581-4975-8ea5-243233fd8643",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf mistral-lora-multi-adapter\n",
    "!mkdir -p mistral-lora-multi-adapter/adapters\n",
    "!echo \"Lora Multi Adapter Model\" > mistral-lora-multi-adapter/README.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "01ba041a-e6d6-471e-8916-1400bbd49ee0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dd897b41c6419386de08e3216af8a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840ec685ef6d4013a6c33ed4f9e5f200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.52k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37feeadafa544b5297fd668d4243e64e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/13.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2973a7e3e5684a1f819ea16400df1945",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_config.json:   0%|          | 0.00/539 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0a92d38a0d4082b36007e91fd43fb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adapter_model.bin:   0%|          | 0.00/85.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'/home/sagemaker-user/sagemaker-genai-hosting-examples/LORA-adapters-IC/mistral-lora-multi-adapter/adapters/fr'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snapshot_download(\"CATIE-AQ/mistral7B-FR-InstructNLP-LoRA\", local_dir=\"mistral-lora-multi-adapter/adapters/fr\", local_dir_use_symlinks=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "263945dd-2703-4215-901d-a26bf6e75f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./\n",
      "./adapters/\n",
      "./adapters/fr/\n",
      "./adapters/fr/.huggingface/\n",
      "./adapters/fr/.huggingface/.gitignore\n",
      "./adapters/fr/.huggingface/download/\n",
      "./adapters/fr/.huggingface/download/.gitattributes.lock\n",
      "./adapters/fr/.huggingface/download/README.md.lock\n",
      "./adapters/fr/.huggingface/download/adapter_config.json.lock\n",
      "./adapters/fr/.huggingface/download/adapter_model.bin.lock\n",
      "./adapters/fr/.huggingface/download/.gitattributes.metadata\n",
      "./adapters/fr/.huggingface/download/adapter_config.json.metadata\n",
      "./adapters/fr/.huggingface/download/README.md.metadata\n",
      "./adapters/fr/.huggingface/download/adapter_model.bin.metadata\n",
      "./adapters/fr/.gitattributes\n",
      "./adapters/fr/adapter_config.json\n",
      "./adapters/fr/README.md\n",
      "./adapters/fr/adapter_model.bin\n",
      "./README.txt\n"
     ]
    }
   ],
   "source": [
    "!rm -f adapers.tar.gz\n",
    "!rm -rf mistral-lora-multi-adapter/.ipynb_checkpoints\n",
    "!tar czvf adapters.tar.gz -C mistral-lora-multi-adapter ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09bd0bd5-ad6b-428e-8092-5cd4fe0ac96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_artifact_accelerate = sess.upload_data(\"adapters.tar.gz\", model_bucket, s3_code_prefix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef3c95f-abf1-4673-b618-3a4c8f42f609",
   "metadata": {},
   "source": [
    "Note: mistralai/Mistral-7B-v0.1 is gated. Access to model mistralai/Mistral-7B-v0.1 is restricted and you must be in the authorized list to use it. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "33c81ed9-7a04-4ee4-a94d-91b7ab4189b2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "deepspeed_image_uri = image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\",\n",
    "    region=sess.boto_session.region_name,\n",
    "    version=\"0.27.0\"\n",
    ")\n",
    "\n",
    "my_hf_token = \"<YOUR_HuggingFacePersonalAccessToken_HERE>\"\n",
    "my_hf_token = \"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\"\n",
    "\n",
    "env_generation = { #\"HUGGINGFACE_HUB_CACHE\": \"/tmp\",\n",
    "                  \"HF_TOKEN\": my_hf_token,\n",
    "                  \"SERVING_LOAD_MODELS\": \"test::Python=/opt/ml/model\",\n",
    "                  \"OPTION_MODEL_ID\": \"mistralai/Mistral-7B-v0.1\",\n",
    "                  \"OPTION_TRUST_REMOTE_CODE\": \"true\",\n",
    "                  \"OPTION_TENSOR_PARALLEL_DEGREE\": \"1\",\n",
    "                  \"OPTION_ENABLE_LORA\": \"true\",\n",
    "                  \"OPTION_GPU_MEMORY_UTILIZATION\": \"0.8\",\n",
    "                  \"OPTION_MAX_LORA_RANK\": \"64\",\n",
    "                  \"OPTION_MAX_CPU_LORAS\": \"4\"\n",
    "                 }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e86989a2-25d1-4f3b-be80-b73219a0698d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables are ---- > {'HF_TOKEN': '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', 'SERVING_LOAD_MODELS': 'test::Python=/opt/ml/model', 'OPTION_MODEL_ID': 'mistralai/Mistral-7B-v0.1', 'OPTION_TRUST_REMOTE_CODE': 'true', 'OPTION_TENSOR_PARALLEL_DEGREE': '1', 'OPTION_ENABLE_LORA': 'true', 'OPTION_GPU_MEMORY_UTILIZATION': '0.8', 'OPTION_MAX_LORA_RANK': '64', 'OPTION_MAX_CPU_LORAS': '4'}\n",
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.27.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "# - Select the appropriate environment variable which will tune the deployment server.\n",
    "env = env_generation # use this in case it is 'generation' task \n",
    "# - now we select the appropriate container \n",
    "inference_image_uri = deepspeed_image_uri # use this in case it is 'generation' task \n",
    "# inference_image_uri = trtllm_image_uri # enable this in case your use case is summarization ( high input and medium output sizes ) \n",
    "\n",
    "print(f\"Environment variables are ---- > {env}\")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af47576b-858c-424d-b5fe-431571561ea7",
   "metadata": {},
   "source": [
    "### Create an inference component/model for Mistral 7B\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "dde9d41f-9116-4350-867d-c432e9e78e61",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmi-mistral-7b-2024-05-14-03-16-09-738\n",
      "Created Model: arn:aws:sagemaker:us-east-1:972812897072:model/lmi-mistral-7b-2024-05-14-03-16-09-738\n"
     ]
    }
   ],
   "source": [
    "model_name2 = sagemaker.utils.name_from_base(\"lmi-mistral-7b\")\n",
    "print(model_name2)\n",
    "\n",
    "create_model_response = sm_client.create_model(\n",
    "    ModelName=model_name2,\n",
    "    ExecutionRoleArn=role,\n",
    "    PrimaryContainer={\n",
    "        \"Image\": inference_image_uri,\n",
    "        \"Environment\": env,\n",
    "        \"ModelDataUrl\": s3_code_artifact_accelerate,\n",
    "    }\n",
    ")\n",
    "model_arn = create_model_response[\"ModelArn\"]\n",
    "\n",
    "print(f\"Created Model: {model_arn}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "884f4a2b-8ec8-493a-a962-9b64abb5ecf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Demo inference component name: lmi-mistral-7b-1715656570-1c78-inference-component:: endpoint_name=lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint\n"
     ]
    }
   ],
   "source": [
    "prefix = sagemaker.utils.unique_name_from_base(\"lmi-mistral-7b\")\n",
    "\n",
    "inference_component_name2 = f\"{prefix}-inference-component\"\n",
    "print(f\"Demo inference component name: {inference_component_name2}:: endpoint_name={endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb8787-1641-4798-a613-8bf83b7740ba",
   "metadata": {},
   "source": [
    "Note: mistralai/Mistral-7B-v0.1 is gated.  Access to model mistralai/Mistral-7B-v0.1 is restricted and you must be in the authorized list to use it. Visit https://huggingface.co/mistralai/Mistral-7B-v0.1 to ask for access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0096a333-6203-4102-a4b6-0ea994d8414c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'InferenceComponentArn': 'arn:aws:sagemaker:us-east-1:972812897072:inference-component/lmi-mistral-7b-1715656570-1c78-inference-component',\n",
       " 'ResponseMetadata': {'RequestId': '415035a6-7725-4c20-9ab7-b10d49ba4096',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '415035a6-7725-4c20-9ab7-b10d49ba4096',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'content-length': '139',\n",
       "   'date': 'Tue, 14 May 2024 03:16:12 GMT'},\n",
       "  'RetryAttempts': 0}}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.create_inference_component(\n",
    "    InferenceComponentName=inference_component_name2,\n",
    "    EndpointName=endpoint_name,\n",
    "    VariantName=variant_name,\n",
    "    Specification={\n",
    "        \"ModelName\": model_name2,\n",
    "        # \"Container\": {\n",
    "        #     \"Image\": inference_image_uri,\n",
    "        #     \"ArtifactUrl\": s3_code_artifact,\n",
    "        # },\n",
    "        \"StartupParameters\": {\n",
    "            \"ModelDataDownloadTimeoutInSeconds\": 3600,\n",
    "            \"ContainerStartupHealthCheckTimeoutInSeconds\": 1200,\n",
    "        },\n",
    "        \"ComputeResourceRequirements\": {\n",
    "            \"NumberOfAcceleratorDevicesRequired\": 1,\n",
    "            \"MinMemoryRequiredInMb\": 7*2*1024,\n",
    "            # \"MinMemoryRequiredInMb\": 1024,\n",
    "        },\n",
    "    },\n",
    "    RuntimeConfig={\"CopyCount\": 1},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9fa5505a-f17c-4d99-b7d7-3aa48e39058a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lmi-mistral-7b-1715656570-1c78-inference-component'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference_component_name2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6038a-7b8f-4d7e-abf3-e3f3834d95c1",
   "metadata": {},
   "source": [
    "#### This step can take ~15 mins or longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c367fab0-82f7-4a6b-9580-9acaff32b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: Creating\n",
      "Status: InService\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "while True:\n",
    "    desc = sm_client.describe_inference_component(\n",
    "        InferenceComponentName=inference_component_name2\n",
    "    )\n",
    "    status = desc[\"InferenceComponentStatus\"]\n",
    "    print(\"Status: \" + status)\n",
    "    sys.stdout.flush()\n",
    "    if status in [\"InService\", \"Failed\"]:\n",
    "        break\n",
    "    time.sleep(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c04c568-636d-45b0-9726-243818eda509",
   "metadata": {},
   "source": [
    "### Invoke the endpoint with a sample prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0762b9-b4c0-44c8-9a1a-032cdd26f848",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = { \"max_new_tokens\": 100}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c3db4077-fe1f-459b-9380-60f9ff327fc8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModelError",
     "evalue": "An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from TPYNTkar7bxmTljCro42FHJPsiEqvzQZ696F with message \"Your invocation timed out while waiting for a response from container TPYNTkar7bxmTljCro42FHJPsiEqvzQZ696F. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint in account 972812897072 for more information.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModelError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:565\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    562\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    563\u001b[0m     )\n\u001b[1;32m    564\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 565\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/botocore/client.py:1021\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1017\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQueryErrorCode\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1018\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1019\u001b[0m     )\n\u001b[1;32m   1020\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mModelError\u001b[0m: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from TPYNTkar7bxmTljCro42FHJPsiEqvzQZ696F with message \"Your invocation timed out while waiting for a response from container TPYNTkar7bxmTljCro42FHJPsiEqvzQZ696F. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\". See https://us-east-1.console.aws.amazon.com/cloudwatch/home?region=us-east-1#logEventViewer:group=/aws/sagemaker/Endpoints/lmi-llama2-7b-2024-05-14-02-51-09-214-endpoint in account 972812897072 for more information."
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Testing French (fr) adapter\n",
    "\n",
    "response_model = smr_client.invoke_endpoint(\n",
    "    InferenceComponentName=inference_component_name2,\n",
    "    EndpointName=endpoint_name,\n",
    "    Body=json.dumps({\"inputs\": [\"Pensez à une excuse créative pour dire que je n'ai pas besoin d'aller à la fête.\"],\n",
    "                     \"parameters\": params,\n",
    "                     \"adapters\": [\"fr\"]}),\n",
    "    ContentType=\"application/json\",\n",
    ")\n",
    "\n",
    "response_model[\"Body\"].read().decode(\"utf8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "317a8135-a522-45c5-8b6b-adec54737f7b",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "18e7f0b2-2d78-452b-89ba-c92d2178d924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ResponseMetadata': {'RequestId': '6103241f-3a92-4513-8b96-8234074fc28f',\n",
       "  'HTTPStatusCode': 200,\n",
       "  'HTTPHeaders': {'x-amzn-requestid': '6103241f-3a92-4513-8b96-8234074fc28f',\n",
       "   'content-type': 'application/x-amz-json-1.1',\n",
       "   'date': 'Tue, 14 May 2024 03:32:56 GMT',\n",
       "   'content-length': '0'},\n",
       "  'RetryAttempts': 3}}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm_client.delete_inference_component(InferenceComponentName=inference_component_name)\n",
    "sm_client.delete_inference_component(InferenceComponentName=inference_component_name2)\n",
    "sm_client.delete_endpoint(EndpointName=endpoint_name)\n",
    "sm_client.delete_endpoint_config(EndpointConfigName=endpoint_config_name)\n",
    "sm_client.delete_model(ModelName=model_name)\n",
    "sm_client.delete_model(ModelName=model_name2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29acba11-80fd-4e2b-b4e7-984e644f8309",
   "metadata": {},
   "source": [
    "#### Resource:\n",
    "- [Deep Learning containers for large model inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-dlc.html)\n",
    "- [Deep Java Library for Large Model Inference](https://docs.djl.ai/docs/serving/serving/docs/large_model_inference.html)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
