{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba48afe-14e8-41f0-9041-03544b2b4064",
   "metadata": {},
   "source": [
    "# Host mixtral-8x7B model on Amazon SageMaker using LMI(TensorRTLLM) container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89f45721-3658-418d-825f-d218fe80f2c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "distributed 2022.7.0 requires tornado<6.2,>=6.0.3, but you have tornado 6.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker --upgrade  --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3a2dc3-d9ee-42fe-8c37-b66ffed13985",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import the relevant libraries and configure global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6678e49c-0b42-4a87-88ff-a0754bcfa674",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import json\n",
    "import io\n",
    "import numpy as np\n",
    "from sagemaker import Model, image_uris, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "session = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = session._region_name  # region name of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610ebbb9-d0a5-4147-bce1-130bc259544e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create serving.properties file, upload model artifacts to S3 and specify the inference container\n",
    "\n",
    "SageMaker Large Model Inference (LMI) containers can be used to host models without any additional inference code. You can configure the model server either through the environment variables or a serving.properties file.  Optionally, we could have like a model.py for any pre or post processing and a requirements.txt file for any additional packages that are required to be installed.\n",
    "\n",
    "In this case we use the serving.properties file to configure the parameters and customize the LMI container behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "042f09aa-0a4c-4f6e-a95c-8871d8832cce",
   "metadata": {},
   "source": [
    "#### Create serving.properties \n",
    "Next, we create the serving.properties configuration file and specify the following settings:\n",
    "\n",
    "\n",
    "- `engine`: The engine for DJL to use. \n",
    "- `option.model_id`: This can be the S3 uri of the pre-trained model or the model id of a pretrained model hosted inside a model repository on huggingface.co (https://huggingface.co/models). In this case, we provide the model id for the Mixtral-8x7B model.\n",
    "- `option.tensor_parallel_degree`: Set to the number of GPU devices over which Accelerate needs to partition the model. This parameter also controls the no of workers per model which will be started up when DJL serving runs. We set this value to \"max\"(maximum GPU on the current machine).\n",
    "- `option.max_rolling_batch_size`: Sets the maximum size of the continuous batch, defining how many sequences can be processed in parallel at any given time  \n",
    "- `option.rolling_batch`: Set to enable continuous batching to optimize accelerator utilization and overall throughput.\n",
    "- `option.model_loading_timeout` : Sets the timeout value for downloading and loading the model to serve inference\n",
    "\n",
    "For more details on the configuration options and an exhaustive list, you can refer the documentation - https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a41c5c40-c74c-46f0-b040-d4c5056e3f1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%writefile serving.properties\n",
    "engine=MPI\n",
    "option.model_id=mistralai/Mixtral-8x7B-v0.1\n",
    "option.tensor_parallel_degree=max\n",
    "option.rolling_batch=auto\n",
    "option.max_rolling_batch_size=32\n",
    "option.model_loading_timeout = 7200 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77db2136-9f50-4265-a159-1191f4f71536",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### We package the serving.properties configuration file in the tar.gz format, so that it meets SageMaker hosting requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b1c8aa-e2c3-4cb3-b4f5-e5f099ff7d24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixtral7b-model/\n",
      "mixtral7b-model/serving.properties\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "mkdir mixtral7b-model\n",
    "mv serving.properties mixtral7b-model/\n",
    "tar czvf mixtral7b-model.tar.gz mixtral7b-model/\n",
    "rm -rf mixtral7b-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f564490b-f628-48e6-b20d-98ed2f81c8ad",
   "metadata": {},
   "source": [
    "#### Configure the Image URI for the inference container\n",
    "\n",
    "We configure the DJL LMI container with deepspeed as the backend engine. Also note that we are specifying the latest version of the container (0.26.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "815449e6-2b95-4ec2-9113-b596d8450740",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_uri = image_uris.retrieve(\n",
    "        framework=\"djl-tensorrtllm\",\n",
    "        region=session.boto_session.region_name,\n",
    "        version=\"0.26.0\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e60454-e1c5-4e2b-b784-4efae2bdbbba",
   "metadata": {},
   "source": [
    "#### Next we upload the local tarball (containing the serving.properties configuration file) to an S3 prefix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30381a3-da57-4501-836b-a0a958f34543",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = session.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = session.upload_data(\"mixtral7b-model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ebea68-5ee8-4c44-858f-b8a23e7dd771",
   "metadata": {},
   "source": [
    "## Create the SageMaker model object and Deploy the Model with the LMI container\n",
    " \n",
    "We use the image URI for the DJL container and the s3 location to which the model serving artifacts tarball were uploaded, to create the SageMaker model object.\n",
    "\n",
    "The container downloads the model into the `/tmp` space on the container because SageMaker maps the `/tmp` to the Amazon Elastic Block Store (Amazon EBS) volume that is mounted when we specify the endpoint creation parameter VolumeSizeInGB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34dba548-eb12-4a80-9882-8f835f89b215",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Model(image_uri=image_uri, model_data=code_artifact, role=role)\n",
    "\n",
    "instance_type = \"ml.p4d.24xlarge\"\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"mixtral-lmi-model\")\n",
    "\n",
    "model.deploy(initial_instance_count=1,\n",
    "             instance_type=instance_type,\n",
    "             endpoint_name=endpoint_name,\n",
    "             VolumeSizeInGB =30,\n",
    "             container_startup_health_check_timeout=1800\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87c24bb-d832-4bdc-979a-99e2551c00ba",
   "metadata": {},
   "source": [
    "#### Note: Please ensure that the size of the mount is large enough to hold the model using the VolumeSizeInGB configuration above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0bec8a-596d-438c-a38f-4c10e629cba6",
   "metadata": {},
   "source": [
    "## Generating log_prob and finish_reason as additional details as part of the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab473b13-ea95-4ef0-98b1-d31f1455bedf",
   "metadata": {},
   "source": [
    "As part of LMI 0.26.0, you can now use 2 additional fine grained details about the generated output i.e. log_prob and finish_reason. \n",
    "\n",
    "* log_probs - the log probability assigned by the model for each token in the streamed sequence chunk. You can use these as a rough estimate of model confidence by computing the joint probability of a sequence as the sum of the log probabilities of the individual tokens, which can be useful for scoring and ranking model outputs. Be mindful that LLM token probabilities are generally overconfident without calibration.\n",
    "\n",
    "* finish_reason - the reason for generation completion, which can be reaching the maximum generation length, generation an end-of-sentence (EOS) toke, or generating a user-defined stop token. This is returned with the last streamed sequence chunk. \n",
    "\n",
    "\n",
    "You can enable these by passing 'details'=True as part of your input to the model. Next, let's see how you can generate these details and we also use a content generation example to understand their application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc6ea64-902f-48e3-883d-221284d476ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper functions for processing streaming response from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e705a24-df00-48ab-9413-f1c5de8378f1",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, we define a LineIterator class, which has functions to lazily fetch bytes from a response stream, buffer them and breakdown the buffer into lines. The idea is to serve bytes from the buffer while fetching more bytes from stream asynchronously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad218c65-3458-409d-be3a-0163b073c751",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LineIterator:\n",
    "\n",
    "    def __init__(self, stream):\n",
    "        # Iterator to get bytes from stream \n",
    "        self.byte_iterator = iter(stream)  \n",
    "        # Buffer stream bytes until we get a full line\n",
    "        self.buffer = io.BytesIO()  \n",
    "      # Track current reading position within buffer\n",
    "        self.read_pos = 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        # Make class iterable \n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        while True:\n",
    "           # Seek read position within buffer\n",
    "           self.buffer.seek(self.read_pos)  \n",
    "           # Try reading a line from current position\n",
    "           line = self.buffer.readline()\n",
    "           # If we have a full line\n",
    "           if line and line[-1] == ord('\\n'):\n",
    "               # Increment reading position past this line\n",
    "               self.read_pos += len(line)  \n",
    "               # Return the line read without newline char\n",
    "               return line[:-1] \n",
    "           # Fetch next chunk from stream  \n",
    "           try:\n",
    "               chunk = next(self.byte_iterator)\n",
    "           # Handle end of stream \n",
    "           except StopIteration:\n",
    "               # Check if we have any bytes still unread\n",
    "               if self.read_pos < self.buffer.getbuffer().nbytes:\n",
    "                   continue\n",
    "               # If not, raise StopIteration\n",
    "               raise\n",
    "           # Add fetched bytes to end of buffer\n",
    "           self.buffer.seek(0, io.SEEK_END)  \n",
    "           self.buffer.write(chunk['PayloadPart']['Bytes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85081105-3b50-4da2-97d6-3c12aeac682f",
   "metadata": {
    "tags": []
   },
   "source": [
    "### log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e98b16-a895-43d6-847c-34e2a33af8a8",
   "metadata": {},
   "source": [
    "Next, consider a use case where we are generating content. Specifically, let's assume that we are tasked writing a brief paragraph about benefits of exercising regularly for a lifestyle focused website. Additionally, we not only want to generate content but also output some indicative score of the confidence that the model has in the generated content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4d8acb1-3a02-4869-aa05-5542fa022016",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=\"\"\"Your task is to write a short paragraph in about 100 words about exercising regularly for a lifestyle focused website. Discuss benefits of regular exercises along with some tips for increasing exercise effectiveness\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb94aa8e-5a1e-4d12-87fe-96da0a82a389",
   "metadata": {},
   "source": [
    "We invoke the model endpoint with our prompt and capture the generated response. Notice that we set details: True as a runtime parameter within the input to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c28c4-e8a5-4588-a81b-2e3b144e4cd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "# set details: True as a runtime parameter within the input.\n",
    "body = {\"inputs\": prompt, \"parameters\": {\"max_new_tokens\":512, \"details\": True}}\n",
    "resp = sm_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(body), ContentType=\"application/json\")\n",
    "event_stream = resp['Body']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2accbc41-1ccf-47a9-b454-361f34dc39db",
   "metadata": {},
   "source": [
    "Since the log probability is generated for each output token, we append the individual log probabilities to a list. We also capture the complete generated text from the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220421bc-b2dc-42f4-be30-10f3e8c3ca59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "overall_log_prob = []\n",
    "\n",
    "for line in LineIterator(event_stream):\n",
    "    resp = json.loads(line)\n",
    "    if resp['token'].get('text') != None:\n",
    "        token_log_prob = resp['token']['log_prob']\n",
    "        overall_log_prob.append(token_log_prob)\n",
    "    elif resp['generated_text'] != None:\n",
    "        generated_text= resp['generated_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a300bda2-97ae-433b-b3ca-4ab4509e889f",
   "metadata": {},
   "source": [
    "Now to calculate the overall confidence score, we calculate the mean of all the individual token probabilities and subsequently get the exponential value between 0 and 1.\n",
    "\n",
    "This is our inferred overall confidence score for the generated text, which in this case is a paragraph about benefits of exercising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2040f60-f687-4dd6-9351-eec7f8f8e29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(generated_text)\n",
    "overall_score=np.exp(np.mean(overall_log_prob))      \n",
    "print(f\"\\n\\nOverall confidence score in the generated text: {overall_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d680317-ec07-4639-afe8-2af43c38176e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### finish_reason"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ca412-45e0-43bd-b4d7-1589f349bafc",
   "metadata": {},
   "source": [
    "Now lets build on the same use case, but lets assume that we are tasked with writing a longer article about benefits of exercising regularly for a lifestyle focused website. Additionally, we want to ensure that the output is not truncated due to generation length issues(max token length) or due to stop tokens being encountered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f574fb4-cb54-4f83-8b62-f2605635ac6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt=\"\"\"Your task is to write a paragraph in about 500 words about exercising regularly for a lifestyle focused website. Discuss benefits of regular exercises along with some tips for increasing exercise effectiveness while reducing required time commitment\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592bda0-cfec-45ae-9bdb-26415f259623",
   "metadata": {},
   "source": [
    "To accomplish this, we use the finish_reason attribute generated in the output, monitor its value and continue generating, until the entire output is generated.\n",
    "\n",
    "1. We define an `inference` function that takes a `payload` input and calls the SageMaker endpoint, streams back a response, and processes the response to extract generated text.\n",
    "\n",
    "2. The `payload` contains the prompt text as `inputs` and parameters like max tokens and details.\n",
    "\n",
    "3. The response is read in a stream and processed line-by-line to extract the generated text tokens into a list. we extract details like `finish_reason`.\n",
    "\n",
    "4. We call the `inference` function in a loop(chained requests) while adding more context each time, and track number of tokens generated and number of requests sent until the model finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80162d0a-2232-4eef-a346-0344ffe1a1c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def inference(payload):\n",
    "    # Call SageMaker endpoint and get response stream\n",
    "    resp = sm_client.invoke_endpoint_with_response_stream(EndpointName=endpoint_name, Body=json.dumps(payload), ContentType=\"application/json\")\n",
    "    event_stream = resp['Body']\n",
    "    text_output = []\n",
    "    for line in LineIterator(event_stream):\n",
    "        resp = json.loads(line) \n",
    "        # Extract text tokens if present\n",
    "        if resp['token'].get('text') != None:\n",
    "            token = resp['token']['text']\n",
    "            text_output.append(token)  \n",
    "            print(token, end='')\n",
    "        # Get finish reason if details present\n",
    "        if resp.get('details') != None:\n",
    "            finish_reason = resp['details']['finish_reason']\n",
    "            # Return extracted output, finish reason and token length\n",
    "            return payload['inputs'] + ''.join(text_output), finish_reason, len(text_output)\n",
    "\n",
    "# set details: True as a runtime parameter within the input.\n",
    "payload = {\"inputs\": prompt,  \"parameters\": {\"max_new_tokens\":256, \"details\": True}} \n",
    "\n",
    "finish_reason = \"length\"\n",
    "# Print initial output \n",
    "print(f\"Output: {payload['inputs']}\", end='')  \n",
    "total_tokens = 0\n",
    "total_requests = 0\n",
    "while finish_reason == 'length':\n",
    "    # Call inference and get extracts\n",
    "    output_text, finish_reason, out_token_len = inference(payload)\n",
    "    # Update payload for next request\n",
    "    payload['inputs'] = output_text \n",
    "    total_tokens += out_token_len\n",
    "    total_requests += 1\n",
    "# Print metrics\n",
    "print(f\"\\n\\ntotal tokens generated: {total_tokens} \\ntotal requests sent: {total_requests}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d43833d-18b8-46bd-a12c-28187b11c29c",
   "metadata": {},
   "source": [
    "As we see above, even though the `max_new_token`paramater is set to 256, we use the `finish_reason` detail attribute as part of the output to chain multiple requests to the endpoint, until the entire output is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bac2026-2555-41d8-94c6-725e652b73d9",
   "metadata": {},
   "source": [
    "## Cleanup the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c9f8e-130d-4aa0-a749-0a4d35665398",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session.delete_endpoint(endpoint_name)\n",
    "session.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff5d980-e64e-4e02-9c2a-42a1f4656c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
