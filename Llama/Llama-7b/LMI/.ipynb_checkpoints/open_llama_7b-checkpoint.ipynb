{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71a329f0",
   "metadata": {},
   "source": [
    "# Open-LLAMA 7B implementation using LMI container on SageMaker\n",
    "\n",
    "\n",
    "#### Model source: https://github.com/openlm-research/open_llama ; \n",
    "#### Model download hub: https://huggingface.co/openlm-research/open_llama_7b; \n",
    "#### License: Apache-2.0\n",
    "\n",
    "\n",
    "In this tutorial, you will bring your own container from docker hub to SageMaker and run inference with it.\n",
    "Please make sure the following permission granted before running the notebook:\n",
    "\n",
    "- ECR Push/Pull access\n",
    "- S3 bucket push access\n",
    "- SageMaker access\n",
    "\n",
    "\n",
    "## Step 1: Let's bump up SageMaker and import stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce21e78b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67fa3208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sagemaker boto3 awscli --upgrade --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec9ac353",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.0' or newer of 'numexpr' (version '2.7.3' currently installed).\n",
      "  from pandas.core.computation.check import NUMEXPR_INSTALLED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/ec2-user/.config/sagemaker/config.yaml\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import Model, serializers, deserializers\n",
    "\n",
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "region = sess._region_name  # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f195c0b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arn:aws:iam::171503325295:role/test-ecr us-east-1 171503325295\n"
     ]
    }
   ],
   "source": [
    "print(role, region, account_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d4667f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.208.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71542f98",
   "metadata": {},
   "source": [
    "## Step 2 Image URI for the DJL container is being used here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7305a5f1-da7c-410d-a0f3-88a70fc53630",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image going to be used is ---- > 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.26.0-deepspeed0.12.6-cu121\n"
     ]
    }
   ],
   "source": [
    "inference_image_uri = sagemaker.image_uris.retrieve(\n",
    "    framework=\"djl-deepspeed\", region=region, version=\"0.26.0\"\n",
    ")\n",
    "print(f\"Image going to be used is ---- > {inference_image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81deac79",
   "metadata": {},
   "source": [
    "## Step 3: Start preparing model artifacts\n",
    "In LMI container, we expect some artifacts to help set up the model.\n",
    "Either enviroment variables or a `serving.properties` file is required. \n",
    "\n",
    "- enviroment variables | serving.properties (required): Defines the model server settings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b27758e-cb2c-452f-add3-81330c06a483",
   "metadata": {},
   "source": [
    "```\n",
    "%%writefile serving.properties\n",
    "engine = MPI\n",
    "option.tensor_parallel_degree = max\n",
    "option.model_id = openlm-research/open_llama_7b\n",
    "option.rolling_batch=vllm\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88ed337b-6533-4dd4-976d-0c43b45ccbad",
   "metadata": {
    "tags": []
   },
   "source": [
    "```\n",
    "%%sh\n",
    "mkdir mymodel\n",
    "mv serving.properties mymodel/\n",
    "tar czvf mymodel.tar.gz mymodel/\n",
    "rm -rf mymodel\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adb55c1-172d-4010-9b8d-e008bb848325",
   "metadata": {},
   "source": [
    "```\n",
    "s3_code_prefix = \"large-model-lmi/code\"\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "code_artifact = sess.upload_data(\"mymodel.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e58cf33",
   "metadata": {},
   "source": [
    "## Step 4: Start building SageMaker endpoint\n",
    "In this step, we will build SageMaker endpoint from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004f39f6",
   "metadata": {},
   "source": [
    "### 4.1 Create SageMaker endpoint\n",
    "We will use enviroment variables to specify the LMI config. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faf35dda-f6d1-4674-aeae-f8cbe04fe148",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "env = {\n",
    "    \"SERVING_LOAD_MODELS\": \"test::MPI=/opt/ml/model\",\n",
    "    \"OPTION_TENSOR_PARALLEL_DEGREE\": \"max\",\n",
    "    \"OPTION_MODEL_ID\": \"openlm-research/open_llama_7b\",\n",
    "    \"OPTION_ROLLING_BATCH\": \"vllm\"\n",
    "}\n",
    "\n",
    "model = Model(image_uri=inference_image_uri, \n",
    "              role=role,\n",
    "              sagemaker_session=sess,\n",
    "             # model_data=code_artifact, # Required only if we are using serving.properties / model.py\n",
    "              env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3557c3a-1acd-43f6-aed5-fcc79d7371dd",
   "metadata": {},
   "source": [
    "You need to specify the instance type to use and endpoint names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e0e61cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "instance_type = \"ml.g5.2xlarge\"  # \"ml.g5.2xlarge\" - #single GPU.\n",
    "\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"open-llama-lmi-model\")\n",
    "\n",
    "model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    container_startup_health_check_timeout=900,\n",
    "    sagemaker_session=sess\n",
    ")\n",
    "\n",
    "# our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "predictor = sagemaker.Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    sagemaker_session=sess,\n",
    "    serializer=serializers.JSONSerializer(),\n",
    "    deserializer=deserializers.JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb63ee65",
   "metadata": {},
   "source": [
    "## Step 5a: Test and benchmark inference latency\n",
    "### The latency is heavily dependent on 'max_new_tokens' parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcef095",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tic = time.time()\n",
    "predictor.predict(\n",
    "    {\"inputs\": \"tuna sandwich nutritional content is \", \"parameters\": {\"max_new_tokens\": 16}}\n",
    ")\n",
    "toc = time.time()\n",
    "print(toc - tic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807e258b",
   "metadata": {},
   "source": [
    "## Let us define a helper function to get a histogram of invocation latency distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b35a270",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def _latency_hist_plot(endpoint_name, invocation_number=100, sleep_time=1):\n",
    "    latency_array = []\n",
    "    for i in tqdm(range(invocation_number)):\n",
    "        tic = time.time()\n",
    "        response_ = predictor.predict(\n",
    "            {\"inputs\": \"Large model inference is\", \"parameters\": {\"max_new_tokens\": 256}}\n",
    "        )\n",
    "        toc = time.time()\n",
    "        latency_array.append(toc - tic)\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    latency_array_np = np.array(latency_array)\n",
    "    _ = plt.hist(latency_array_np, bins=\"auto\")  # arguments are passed to np.histogram\n",
    "    plt.title(\"Invocation Latency Histogram with 'auto' bins\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c7bd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "inv_start_time = time.time()\n",
    "invocation_number = 10\n",
    "# Real-time endpoint\n",
    "_latency_hist_plot(endpoint_name, invocation_number, sleep_time=1)\n",
    "inv_lapse_time = time.time() - inv_start_time\n",
    "print(inv_lapse_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53c2f1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endpoint_name = predictor.endpoint_name\n",
    "print(endpoint_name)\n",
    "print(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1305440",
   "metadata": {},
   "source": [
    "## Step 5b: Analyze Inference Latency via CloudWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b31abc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-test-endpoints.html\n",
    "import pandas as pd\n",
    "\n",
    "cw = boto3.client(\"cloudwatch\", region_name=region)\n",
    "\n",
    "\n",
    "def get_invocation_metrics_for_endpoint(endpoint_name, metric_name, start_time, end_time):\n",
    "    #     metric = \"Sum\"\n",
    "    metric = \"Average\"\n",
    "    metrics = cw.get_metric_statistics(\n",
    "        Namespace=\"AWS/SageMaker\",\n",
    "        MetricName=metric_name,\n",
    "        StartTime=start_time,\n",
    "        EndTime=end_time,\n",
    "        Period=1,\n",
    "        Statistics=[metric],\n",
    "        Dimensions=[\n",
    "            {\"Name\": \"EndpointName\", \"Value\": endpoint_name},\n",
    "            {\"Name\": \"VariantName\", \"Value\": \"AllTraffic\"},\n",
    "        ],\n",
    "    )\n",
    "    return (\n",
    "        pd.DataFrame(metrics[\"Datapoints\"])\n",
    "        .sort_values(\"Timestamp\")\n",
    "        .set_index(\"Timestamp\")\n",
    "        .drop(\"Unit\", axis=1)\n",
    "        .rename(columns={metric: metric_name})\n",
    "    )\n",
    "\n",
    "\n",
    "#     return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e16745",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def plot_endpoint_metrics(start_time=None, end_time=None):\n",
    "    #    start_time = start_time or datetime.datetime.now() - datetime.timedelta(seconds=inv_lapse_time+60)\n",
    "    #    end_time = datetime.datetime.now()\n",
    "    model_metrics = get_invocation_metrics_for_endpoint(\n",
    "        endpoint_name, \"ModelLatency\", start_time, end_time\n",
    "    )\n",
    "    overhead_metrics = get_invocation_metrics_for_endpoint(\n",
    "        endpoint_name, \"OverheadLatency\", start_time, end_time\n",
    "    )\n",
    "    total_metrics = model_metrics.join(overhead_metrics)\n",
    "    total_metrics[\"ModelLatency\"] = total_metrics[\"ModelLatency\"] / 1000\n",
    "    total_metrics[\"OverheadLatency\"] = total_metrics[\"OverheadLatency\"] / 1000\n",
    "    #    total_metrics[\"TotalLatency in ms\"] = total_metrics[[\"ModelLatency\",\"OverheadLatency\"]].sum(axis=1)\n",
    "    #     total_metrics = total_metrics.drop(['ModelLatency', 'OverheadLatency'], axis=1)\n",
    "    total_metrics.plot()\n",
    "    return total_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249d41e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "endtime = datetime.datetime.now()\n",
    "print(endtime)\n",
    "startime = endtime - datetime.timedelta(seconds=inv_lapse_time + 60)\n",
    "print(startime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43f54e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# wait for cloudwatch metrics to populate\n",
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3f536a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_metrics = plot_endpoint_metrics(start_time=startime, end_time=endtime)\n",
    "# total_metrics = plot_endpoint_metrics(start_time=startime, end_time=endtime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d10a1d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Latency expressed in ms\n",
    "total_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401d0e40",
   "metadata": {},
   "source": [
    "## Clean up the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5552dd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.delete_endpoint(endpoint_name)\n",
    "sess.delete_endpoint_config(endpoint_name)\n",
    "model.delete_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
