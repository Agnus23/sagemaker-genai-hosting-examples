---
apiVersion: v1
kind: Pod
metadata:
  name: lmi-inference
  labels:
    app: lmi-inference
spec:
  containers:
    - name: lmi-inference
      image: deepjavalibrary/djl-serving:0.27.0-tensorrt-llm
      resources:
        limits:
          nvidia.com/gpu: 2
        requests:
          cpu: "8"
          memory: 8Gi
          nvidia.com/gpu: 2
          ephemeral-storage: 20Gi
      ports:
        - containerPort: 80
          name: http
      volumeMounts:
        - name: model
          mountPath: /opt/ml/model
        - name: shm
          mountPath: /dev/shm
      env:
        - name: HUGGING_FACE_HUB_TOKEN
          value: "ADD Key"
        - name: OPTION_MODEL_ID
          value: "TheBloke/Llama-2-13B-Chat-fp16"
        - name: OPTION_TENSOR_PARALLEL_DEGREE
          value: "2"
  volumes:
    - name: model
      hostPath:
       path: /mnt
       type: DirectoryOrCreate
    - name: shm
      emptyDir:
        medium: Memory
        sizeLimit: 1Gi
  tolerations:
    - key: "nvidia.com/gpu"
      operator: "Exists"
      effect: "NoSchedule"
  restartPolicy: Never
---
apiVersion: v1
kind: Service
metadata:
  name: lmi-inference
spec:
  ports:
    - port: 8080
      protocol: TCP
      targetPort: http
  selector:
    app: text-inference
  type: ClusterIP